{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamwelMakombe-cloud/ce-colab-notebooks/blob/main/Data_Preparation_and_Exploration_for_development_corpus_CLEAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSgYyuxMFen5"
      },
      "source": [
        "### **Installation of required packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdQgMxp7VkuP"
      },
      "outputs": [],
      "source": [
        "!pip install pymupdf pandas\n",
        "!pip install -q pdfplumber pandas tqdm\n",
        "\n",
        "\n",
        "import re, pdfplumber, pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import json\n",
        "import spacy\n",
        "import re\n",
        "import re\n",
        "from typing import Dict\n",
        "import spacy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6jYRZQ2LzL4"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1qdJqDGniWv"
      },
      "source": [
        "### **1. PARSING OF PDFS INTO STRUCTURED CSVS AND CLEANING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cFeCnShnoWC"
      },
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# 🔹 Stage 1-A – Extract raw question-answer rows from every PDF\n",
        "#     • Installs pdfplumber\n",
        "#     • Mounts Google Drive\n",
        "#     • Parses each PDF in /content/drive/MyDrive/project/CB\n",
        "#     • Saves stage1_raw.csv (no preprocessing yet)\n",
        "\n",
        "\n",
        "PDF_DIR   = Path(\"/content/drive/MyDrive/project/CB\")   # folder with 65 PDFs\n",
        "RAW_CSV   = PDF_DIR / \"stage1_raw.csv\"\n",
        "\n",
        "def parse_pdf_table(pdf_path: Path) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    with pdfplumber.open(str(pdf_path)) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            for table in page.extract_tables() or []:\n",
        "                if not table or len(table) < 2:\n",
        "                    continue\n",
        "                # Skip header (row 0)\n",
        "                for q, a, *_ in table[1:]:\n",
        "                    q, a = (q or \"\").strip(), (a or \"\").strip()\n",
        "                    if not q and not a:\n",
        "                        continue\n",
        "                    m = re.match(r\"^(A\\d+(?:\\.\\d+)+)\", q)\n",
        "                    rows.append(\n",
        "                        {\n",
        "                            \"question_id\": m.group(1) if m else None,\n",
        "                            \"question_text\": q,\n",
        "                            \"answer_text\":   a,\n",
        "                            \"file\": pdf_path.name,\n",
        "                        }\n",
        "                    )\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# Parse every PDF, concatenate, save\n",
        "pdfs = sorted(PDF_DIR.glob(\"*.pdf\"))\n",
        "if not pdfs:\n",
        "    raise FileNotFoundError(f\"No PDFs found in {PDF_DIR}\")\n",
        "\n",
        "frames = [parse_pdf_table(p) for p in pdfs]\n",
        "raw_df = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "raw_df.to_csv(RAW_CSV, index=False)\n",
        "print(\"  Saved raw merged CSV:\", RAW_CSV, \"\\n\")\n",
        "display(raw_df.head(20))          # preview first 10 rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byWlu4m6nuEn"
      },
      "outputs": [],
      "source": [
        "# %% Text-regex extractor only  ➜  stage1_raw_text.csv\n",
        "# -----------------------------------------------------\n",
        "!pip install -q pymupdf pandas tqdm\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import fitz, re, os, pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "PDF_DIR  = Path(\"/content/drive/MyDrive/project/CB\")          # your PDF folder\n",
        "TEXT_CSV = PDF_DIR / \"stage1_raw_text.csv\"                    # output file\n",
        "\n",
        "# ── regex helpers ──────────────────────────────────────────────\n",
        "BLOCK_RE = re.compile(\n",
        "    r\"(A\\d+\\.\\d+[A-Za-z0-9\\-]*.*?)\\n(.*?)(?=\\nA\\d+\\.\\d+|\\Z)\", re.DOTALL\n",
        ")\n",
        "NOTE_RE  = re.compile(r\"^\\s*(Notes:|Applicant Notes:)\", re.I)\n",
        "\n",
        "def extract_text(pdf_path: Path) -> str:\n",
        "    with fitz.open(str(pdf_path)) as doc:\n",
        "        return \"\\n\".join(pg.get_text() for pg in doc)\n",
        "\n",
        "def parse_block(qid: str, blob: str) -> dict:\n",
        "    lines = [l.strip() for l in blob.strip().splitlines()]\n",
        "    # first blank line separates heading vs answer; if none, last line = answer\n",
        "    try:\n",
        "        blank = lines.index(\"\")\n",
        "    except ValueError:\n",
        "        blank = len(lines) - 1\n",
        "    q_text  = \" \".join(lines[: blank + 1]).strip()\n",
        "    ans_raw = \" \".join(lines[blank + 1 :]).strip() or lines[-1].strip()\n",
        "\n",
        "    # move Notes into answer\n",
        "    notes, parts = [], []\n",
        "    for seg in ans_raw.split(\"│\") if \"│\" in ans_raw else [ans_raw]:\n",
        "        if NOTE_RE.match(seg):\n",
        "            notes.append(seg.split(\":\", 1)[-1].strip())\n",
        "        else:\n",
        "            parts.append(seg.strip())\n",
        "    answer = \" \".join(parts)\n",
        "    if notes:\n",
        "        answer = (\n",
        "            f\"{answer} │ Notes: {' | '.join(notes)}\" if answer\n",
        "            else f\"Notes: {' | '.join(notes)}\"\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        \"question_id\":   qid.strip(),\n",
        "        \"question_text\": q_text,\n",
        "        \"answer_text\":   answer,\n",
        "    }\n",
        "\n",
        "# ── run over every PDF ────────────────────────────────────────\n",
        "rows = []\n",
        "for fname in tqdm(sorted(os.listdir(PDF_DIR)), desc=\"Extracting text\"):\n",
        "    if fname.lower().endswith(\".pdf\"):\n",
        "        text = extract_text(PDF_DIR / fname)\n",
        "        for qid, block in BLOCK_RE.findall(text):\n",
        "            rec = parse_block(qid, block)\n",
        "            rec[\"file\"] = fname\n",
        "            rows.append(rec)\n",
        "\n",
        "text_df = pd.DataFrame(rows)\n",
        "text_df.to_csv(TEXT_CSV, index=False)\n",
        "print(f\"  Saved text extractor output → {TEXT_CSV}   (rows: {len(text_df)})\")\n",
        "\n",
        "# Preview the first 10 rows\n",
        "display(text_df.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwKaElTJn3Mq"
      },
      "outputs": [],
      "source": [
        "# %% Merge with `source` column, keep table rows in case of duplicates\n",
        "# -------------------------------------------------------------------\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "PDF_DIR   = Path(\"/content/drive/MyDrive/project/CB\")\n",
        "TABLE_CSV = PDF_DIR / \"stage1_raw.csv\"          # first extractor\n",
        "TEXT_CSV  = PDF_DIR / \"stage1_raw_text.csv\"     # second extractor\n",
        "OUT_CSV   = PDF_DIR / \"stage1_raw_merged.csv\"\n",
        "\n",
        "# ── load & tag ------------------------------------------------------\n",
        "table_df = pd.read_csv(TABLE_CSV, dtype=str).fillna(\"\")\n",
        "table_df[\"source\"] = \"table\"                    # mark origin\n",
        "\n",
        "text_df  = pd.read_csv(TEXT_CSV,  dtype=str).fillna(\"\")\n",
        "text_df[\"source\"]  = \"text\"\n",
        "\n",
        "# normalise IDs in text extractor so they match table style\n",
        "text_df[\"question_id\"] = (\n",
        "    text_df[\"question_id\"].str.extract(r\"^\\s*(A\\d+(?:\\.\\d+)+)\")[0].fillna(\"\")\n",
        ")\n",
        "text_df = text_df[text_df[\"question_id\"] != \"\"]  # drop if ID missing\n",
        "\n",
        "# keep common column order\n",
        "cols = [\"file\", \"question_id\", \"question_text\", \"answer_text\", \"source\"]\n",
        "table_df = table_df[cols]\n",
        "text_df  = text_df [cols]\n",
        "\n",
        "# ── concatenate (table first), then drop duplicates ---------------\n",
        "combined = pd.concat([table_df, text_df], ignore_index=True)\n",
        "\n",
        "# keep the first occurrence of each (file, question_id) pair\n",
        "combined_no_dupes = combined.drop_duplicates(\n",
        "    subset=[\"file\", \"question_id\"], keep=\"first\"\n",
        ")\n",
        "\n",
        "# …existing code up to combined_no_dupes …\n",
        "\n",
        "# ▶  sort by file, then question_id\n",
        "combined_no_dupes = combined_no_dupes.sort_values([\"file\", \"question_id\"])\n",
        "\n",
        "combined_no_dupes.to_csv(OUT_CSV, index=False)\n",
        "print(f\" merged file (sorted) → {OUT_CSV}   (rows: {len(combined_no_dupes)})\")\n",
        "display(combined_no_dupes.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIt9PoUFozwe"
      },
      "source": [
        "### Cleaning the merged data csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHf2fLotoa1N"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from pathlib import Path\n",
        "import re, unicodedata\n",
        "\n",
        "MERGED = Path(\"/content/drive/MyDrive/project/CB/stage1_raw_merged.csv\")\n",
        "df = pd.read_csv(MERGED)\n",
        "\n",
        "# Ensure text columns are strings\n",
        "df[\"question_text\"] = df[\"question_text\"].fillna(\"\").astype(str)\n",
        "df[\"answer_text\"] = df[\"answer_text\"].fillna(\"\").astype(str)\n",
        "\n",
        "\n",
        "# --- helper: unify curly/straight quotes & collapse whitespace ---\n",
        "def normalise(txt: str) -> str:\n",
        "    if not txt:\n",
        "        return \"\"\n",
        "    # convert curly quotes to straight ones\n",
        "    tx = txt.translate({\n",
        "        0x201C: '\"', 0x201D: '\"',  # “ ”\n",
        "        0x2018: \"'\", 0x2019: \"'\",  # ‘ ’\n",
        "    })\n",
        "    # collapse runs of whitespace\n",
        "    tx = re.sub(r'\\s+', ' ', tx.strip())\n",
        "    return tx\n",
        "\n",
        "\n",
        "# Helper: Move only if not duplicate\n",
        "def move_if_not_duplicate(source_text: str, to_move: str, target_text: str) -> (str, str):\n",
        "    \"\"\"Moves to_move → target_text if not already present. If present, just remove from source_text.\"\"\"\n",
        "    if to_move.strip() and to_move.strip() in target_text:\n",
        "        return source_text.replace(to_move, \"\").strip(), target_text.strip()\n",
        "    else:\n",
        "        return source_text.replace(to_move, \"\").strip(), f\"{to_move.strip()}\\n{target_text.strip()}\".strip()\n",
        "\n",
        "# Rule 1: Move trailing \"yes\" or \"no\" at the end of question_text\n",
        "def move_trailing_yes_no(row):\n",
        "    qt, at = row[\"question_text\"].strip(), row[\"answer_text\"].strip()\n",
        "    match = re.search(r\"\\b(yes|no)\\s*$\", qt, flags=re.IGNORECASE)\n",
        "    if match:\n",
        "        answer = match.group(1)\n",
        "        qt = qt[:match.start()].strip()\n",
        "        if answer.strip() not in at:\n",
        "            at = f\"{answer}\\n{at}\".strip()\n",
        "    return pd.Series([qt, at])\n",
        "\n",
        "df[[\"question_text\", \"answer_text\"]] = df.apply(move_trailing_yes_no, axis=1)\n",
        "\n",
        "# Rule 2: Move \"yes\"/\"no\" only if followed by whitespace and \"notes\" or nothing (strict)\n",
        "def move_yes_no_and_suffix(row):\n",
        "    qt, at = row[\"question_text\"], row[\"answer_text\"]\n",
        "\n",
        "    # ── match \"yes\" or \"no\" followed by whitespace + \"notes\" / \"Notes:\" ──\n",
        "    m = re.search(r\"\\b(yes|no)\\b\\s*notes?:\", qt, flags=re.IGNORECASE)\n",
        "    if not m:\n",
        "        # also match if yes/no alone is the final token\n",
        "        m = re.search(r\"\\b(yes|no)\\b\\s*$\", qt, flags=re.IGNORECASE)\n",
        "        if not m:\n",
        "            return pd.Series([qt, at])\n",
        "\n",
        "    idx = m.start()                    # start of \"yes\"/\"no\"\n",
        "    block = qt[idx:].lstrip()          # everything from yes/no to the end\n",
        "    qt    = qt[:idx].rstrip()          # keep text before yes/no\n",
        "\n",
        "    # append the full block to answer_text (avoid duplication if already present)\n",
        "    if normalise(block) not in normalise(at):\n",
        "        at = f\"{block}\\n{at}\".strip()\n",
        "\n",
        "    return pd.Series([qt, at])\n",
        "\n",
        "\n",
        "df[[\"question_text\", \"answer_text\"]] = df.apply(move_yes_no_and_suffix, axis=1)\n",
        "\n",
        "# Rule 3: Move text after \".pdf\" to answer_text (if not duplicate)\n",
        "def move_after_pdf(row):\n",
        "    \"\"\"\n",
        "    Move everything after a '.pdf' link into answer_text **only if**\n",
        "    that link is the last non-whitespace content in question_text.\n",
        "    \"\"\"\n",
        "    qt, at = row[\"question_text\"], row[\"answer_text\"]\n",
        "\n",
        "    # Find the last occurrence of \".pdf\"\n",
        "    m = re.search(r\"(?i)(\\.pdf)\", qt)\n",
        "    if not m:\n",
        "        return pd.Series([qt, at])\n",
        "\n",
        "    # anything after .pdf that is not just whitespace?\n",
        "    after = qt[m.end():]\n",
        "    if after.strip():          # there's more text → leave it in question\n",
        "        return pd.Series([qt, at])\n",
        "\n",
        "    # otherwise move the trailing bit (incl. preceding space/newline if any)\n",
        "    qt_main = qt[:m.end()]          # keep the link inside question\n",
        "    # (nothing actually follows .pdf here, but for symmetry we call helper)\n",
        "    qt, at = move_if_not_duplicate(qt_main, \"\", at)\n",
        "    return pd.Series([qt.strip(), at.strip()])\n",
        "\n",
        "\n",
        "# Rule 4: Tail split logic per question_id — move ONLY what comes AFTER the tail to answer_text\n",
        "explanatory_tails = {\n",
        "    \"A2.2\": \"whole organisation excluding development network.\",\n",
        "    \"A2.3\": \"Manchester and Glasgow retail stores).\",\n",
        "    \"A2.4\": \"information.\",\n",
        "    \"A2.4.1\": \"Requirements-for-Infrastructure-v3-1-January-2023.pdf\",\n",
        "    \"A2.5\": \"Redhat Enterprise Linux 8.3\",\n",
        "    \"A2.6\": \"information.\",\n",
        "    \"A2.7\": \"v3-1-January-2023.pdf\",\n",
        "    \"A2.7.1\": \"v3-1-January-2023.pdf\",\n",
        "    \"A2.8\": \"addresses or serial numbers.\",\n",
        "    \"A2.9\": \"Requirements-for-Infrastructure-v3-1-January-2023.pdf\",\n",
        "    \"A2.10\": \"IT provider.\",\n",
        "    \"A4.1.1\": \"included in the operating system of their device.\",\n",
        "    \"A4.2\": \"changed.\",\n",
        "    \"A4.2.1\": \"achieved.\",\n",
        "    \"A4.6\": \"who checks that it has been done?).\",\n",
        "    \"A4.10\": \"Please explain which option is used.\",\n",
        "    \"A4.12\": \"common Linux distributions such as Ubuntu do have software firewalls available.\",\n",
        "    \"A5.1\": \"rpm, yum).\",\n",
        "    \"A5.5\": \"Requirements-for-Infrastructure-v3-1-January-2023.pdf\",\n",
        "    \"A5.9\": \"organisational data or services.\",\n",
        "    \"A5.10\": \"authentication.\",\n",
        "    \"A6.2.1\": \"For example: Chrome Version 102, Safari Version 15.\",\n",
        "    \"A6.1\": \"Most major vendors will have published EOL dates for their operating systems and firmware.\",\n",
        "    \"A6.2.2\": \"2020.\",\n",
        "    \"A6.2.3\": \"For example: MS Exchange 2016, Outlook 2019.\",\n",
        "    \"A6.2.4\": \"For example: MS 365; Libre office, Google workspace, Office 2016.\",\n",
        "    \"A6.4.2\": \"If you only use auto updates, please confirm this in the notes field for this question.\",\n",
        "    \"A6.5.2\": \"If you only use auto updates, please confirm this in the notes field for this question.\",\n",
        "    \"A6.7\": \"by a firewall or VLAN.\",\n",
        "    \"A7.1\": \"are only provided after they have been approved by a person with a leadership role in the business.\",\n",
        "    \"A7.3\": \"When an individual leaves your organisation you need to stop them accessing any of your systems.\",\n",
        "    \"A7.4\": \"folders and applications that they need to do their day to day work.\",\n",
        "    \"A7.5\": \"This process might include approval by a person who is an owner/director/trustee/partner of the organisation.\",\n",
        "    \"A7.6\": \"Cloud service administration must be carried out through separate accounts.\",\n",
        "    \"A7.7\": \"it could be based on good policy, procedure and regular training for staff.\",\n",
        "    \"A7.10\": \"v3-1-January-2023.pdf\",\n",
        "    \"A7.11\": \"v3-1-January-2023.pdf\",\n",
        "    \"A7.12\": \"v3-1-January-2023.pdf\",\n",
        "    \"A4.9\": \"be documented (for example, written down).\",\n",
        "    \"A4.1\": \"network and the internet.\",\n",
        "    \"A4.3\": \"Requirements-for-Infrastructure-v3-1-January-2023.pdf\",\n",
        "    \"A4.4\": \"the password to access the device will need to be changed.\",\n",
        "    \"A4.4\": \"the password to access the device will need to be changed.\",\n",
        "    \"A4.5\": \"most firewalls block all services.\",\n",
        "    \"A4.7\": \"you need to check your firewall settings.\",\n",
        "    \"A5.2\": \"3. Linux using \"\"cat /etc/passwd\"\"\",\n",
        "    \"A5.4\": \"not want to be publicly accessible.\",\n",
        "    \"A5.7\": \"hope of gaining access.\",\n",
        "    \"A5.8\": \"chosen this option, you can answer yes to this question.\",\n",
        "    \"A5.9\": \"organisational data or services.\",\n",
        "    \"A6.1\": \"their operating systems and firmware.\",\n",
        "    \"A6.2\": \"such as Java, Adobe Reader and .NET.\",\n",
        "    \"A6.3\": \"purchased, they will not be receiving regular security updates.\",\n",
        "    \"A6.4\": \"This requirement includes the firmware on your firewalls and routers.\",\n",
        "    \"A6.4.1\": \"This must be enabled on any device where possible.\",\n",
        "    \"A6.5\": \"high-risk or critical security updates.\",\n",
        "    \"A6.5.1\": \"Auto updates should be enabled where possible.\",\n",
        "    \"A6.6\": \"Java and Flash, and all application software.\",\n",
        "    \"A7.2\": \"Accounts must not be shared.\",\n",
        "    \"A7.8\": \"You must track all people that have been granted administrator accounts.\",\n",
        "    \"A7.9\": \"Any users whono longer need administrative access to carry out their role should have it removed.\",\n",
        "    \"A7.14\": \"linked to are Azure, MS365, Google Workspace.\",\n",
        "    \"A7.16\": \"conjunction with a password of at least 8 characters.\",\n",
        "    \"A7.17\": \"conjunction with a password of at least 8 characters.\",\n",
        "    \"A8.1\": \" explanation notes will be required.\",\n",
        "    \"A8.2\": \"doubt, Windows Defender is suitable for this purpose.\",\n",
        "    \"A8.3\": \" access to known malicious websites. On Windows 10, SmartScreen can provide this functionality.\",\n",
        "    \"A8.4\": \"a device to allow unsigned applications.\",\n",
        "    \"A8.5\": \"using good policy, processes and training of staff.\",\n",
        "    \"A4.5.1\": \"level and associated risks reviewed regularly.\",\n",
        "    \"A1.10\": \"information will be kept confidential.\",\n",
        "    \"A5.2\": '3. Linux using \"\"cat /etc/passwd\"\"',\n",
        "    \"A4.11\": '\"Windows firewall\". On Linux try \"ufw status\".',\n",
        "    \"A5.3\":  'Sequences such as \"12345\".',\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "def move_explanatory_tail(row):\n",
        "    qid, qt, at = row[\"question_id\"], row[\"question_text\"], row[\"answer_text\"]\n",
        "    if pd.notna(qid) and qid in explanatory_tails:\n",
        "        tail = explanatory_tails[qid].strip()\n",
        "        idx = qt.find(tail)\n",
        "        if idx != -1:\n",
        "            tail_end = idx + len(tail)\n",
        "            after_tail = qt[tail_end:].strip()\n",
        "            qt = qt[:tail_end].strip()  # Keep tail in question_text\n",
        "            if after_tail:\n",
        "                qt, at = move_if_not_duplicate(qt + \" \" + after_tail, after_tail, at)\n",
        "    return pd.Series([qt.strip(), at.strip()])\n",
        "\n",
        "df[[\"question_text\", \"answer_text\"]] = df.apply(move_explanatory_tail, axis=1)\n",
        "\n",
        "def move_first_word_if_tail_or_pdf(row):\n",
        "    qt, at = row[\"question_text\"].strip(), row[\"answer_text\"].strip()\n",
        "    qid = row[\"question_id\"]\n",
        "\n",
        "    if not at or not pd.notna(qid):\n",
        "        return pd.Series([qt, at])\n",
        "\n",
        "    first_token = at.strip().split()[0]\n",
        "    if qid in explanatory_tails:\n",
        "        tail_last_word = explanatory_tails[qid].strip().split()[-1]\n",
        "        if first_token == tail_last_word or first_token.lower().endswith(\".pdf\"):\n",
        "            if first_token not in qt:\n",
        "                qt = f\"{qt.strip()} {first_token}\".strip()\n",
        "            at = \" \".join(at.strip().split()[1:]).strip()\n",
        "\n",
        "    return pd.Series([qt, at])\n",
        "\n",
        "\n",
        "# Apply tail suffix match from start of answer_text\n",
        "df[[\"question_text\", \"answer_text\"]] = df.apply(move_first_word_if_tail_or_pdf, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "# --- delete any sentence in answer_text that matches tail exactly ---\n",
        "def drop_exact_tail_sentence(row):\n",
        "    qid, at = row[\"question_id\"], row[\"answer_text\"]\n",
        "    if qid not in explanatory_tails or not at.strip():\n",
        "        return pd.Series([row[\"question_text\"], at])\n",
        "\n",
        "    tail_norm = normalise(explanatory_tails[qid])\n",
        "    at_norm   = normalise(at)\n",
        "\n",
        "    # case 1: answer is *only* the tail\n",
        "    if at_norm == tail_norm:\n",
        "        return pd.Series([row[\"question_text\"], \"\"])\n",
        "\n",
        "    # case 2: answer has multiple sentences – remove the one(s) equal to tail\n",
        "    sent_split = re.split(r'(?<=[.!?])\\s+', at.strip())\n",
        "    kept = [s for s in sent_split if normalise(s) and normalise(s) != tail_norm]\n",
        "    new_at = \" \".join(kept).strip()\n",
        "    return pd.Series([row[\"question_text\"], new_at])\n",
        "\n",
        "# apply the drop-tail rule\n",
        "df[[\"question_text\", \"answer_text\"]] = df.apply(drop_exact_tail_sentence, axis=1)\n",
        "\n",
        "\n",
        "# Save cleaned output\n",
        "output_path = \"/content/drive/MyDrive/project/CB/stage1_cleaned_questions.csv\"\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "# remove NaN values\n",
        "df[\"answer_text\"] = df[\"answer_text\"].fillna(\"\")\n",
        "\n",
        "\n",
        "# Preview first 10 rows\n",
        "from IPython.display import display\n",
        "display(df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMkDextHpIxB"
      },
      "source": [
        "### Checking if yes_no questions mapped correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rswJbqnHohEH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the target question IDs\n",
        "target_ids = [\n",
        "    \"A1.9\", \"A1.10\", \"A2.1\", \"A4.1\", \"A4.2\", \"A4.4\", \"A4.5\", \"A4.5.1\",\n",
        "    \"A4.7\", \"A4.8\", \"A4.9\", \"A4.11\", \"A5.2\", \"A5.3\", \"A5.4\", \"A5.8\", \"A5.9\",\n",
        "    \"A6.1\", \"A6.2\", \"A6.3\", \"A6.4\", \"A6.4.1\", \"A6.5\", \"A6.5.1\", \"A6.6\",\n",
        "    \"A7.2\", \"A8.2\", \"A8.3\", \"A8.4\", \"A8.5\", \"A7.16\", \"A7.17\"\n",
        "]\n",
        "\n",
        "# Filter the dataframe\n",
        "filtered = df[\n",
        "    df[\"question_id\"].isin(target_ids) &  # Only target IDs\n",
        "    df[\"answer_text\"].str.strip().ne(\"\") &  # Not blank\n",
        "    ~df[\"answer_text\"].str.lower().str.contains(r\"\\byes\\b|\\bno\\b\")  # Neither \"yes\" nor \"no\"\n",
        "]\n",
        "\n",
        "# Show the first 20 results\n",
        "from IPython.display import display\n",
        "display(filtered.head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpjebRVCpmbr"
      },
      "source": [
        "### Checking if multiple choice questions mapped correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBykX47kpjdl"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# Define the target question IDs\n",
        "target_ids = [\"A5.5\", \"A4.3\", \"A5.7\", \"A8.1\"]\n",
        "\n",
        "# Filter for those question_ids\n",
        "subset = df[df[\"question_id\"].isin(target_ids)]\n",
        "\n",
        "# Keep only rows where answer_text is NOT blank\n",
        "subset = subset[subset[\"answer_text\"].str.strip() != \"\"]\n",
        "\n",
        "# Now check if 'answer_text' does NOT start with A., B., C., or D.\n",
        "invalid_start = ~subset[\"answer_text\"].str.strip().str.startswith(tuple(\"ABCDabcd\"), na=False)\n",
        "\n",
        "# Final filtered rows\n",
        "invalid_rows = subset[invalid_start]\n",
        "\n",
        "# Display the result\n",
        "print(f\"⚠️ Found {len(invalid_rows)} rows where answer_text is non-empty but does NOT start with A./B./C./D.:\")\n",
        "display(invalid_rows[[\"file\", \"question_id\", \"question_text\", \"answer_text\"]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j82eEbZzoKRN"
      },
      "source": [
        "### checking answers with single words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpqvbrjenxoe"
      },
      "outputs": [],
      "source": [
        "# Normalize answer_text\n",
        "df[\"answer_text\"] = df[\"answer_text\"].fillna(\"\").astype(str).str.strip()\n",
        "\n",
        "# Define filter: single word, not yes/no, not blank\n",
        "single_word_answers = df[\n",
        "    df[\"answer_text\"].apply(lambda x: len(x.split()) == 1 and x.lower() not in [\"yes\", \"no\"] and x.strip() != \"\")\n",
        "]\n",
        "\n",
        "# Display first 20 matching rows\n",
        "from IPython.display import display\n",
        "display(single_word_answers.head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4oUf1BhrGoS"
      },
      "source": [
        "### **2. PREPARATION OF DATA FOR MANUAL ANNOTATIONS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xehYEYdwoFW8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "# Load the CSV\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/project/CB/stage1_raw_merged.csv\")\n",
        "\n",
        "# Remove the 'source' column if it exists\n",
        "if 'source' in df.columns:\n",
        "    df.drop(columns=['source'], inplace=True)\n",
        "\n",
        "# Drop rows where 'question_id' is NaN or blank\n",
        "df = df[df['question_id'].notna()]                   # remove NaNs\n",
        "df = df[df['question_id'].astype(str).str.strip() != \"\"]  # remove empty strings\n",
        "\n",
        "# Add a new 'compliance' column with empty values\n",
        "df['compliance'] = \"\"\n",
        "\n",
        "# Save the new dataframe to CSV\n",
        "output_path = \"/content/drive/MyDrive/project/CB/manual_null_compliance.csv\"\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "# Display the first 10 rows\n",
        "display(df.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcLV1wXCutMW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "# Load the original CSV\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/project/CB/stage1_raw_merged.csv\")\n",
        "\n",
        "# Remove the 'source' column if it exists\n",
        "if 'source' in df.columns:\n",
        "    df.drop(columns=['source'], inplace=True)\n",
        "\n",
        "# Drop rows with missing or blank 'question_id'\n",
        "df = df[df['question_id'].notna()]\n",
        "df = df[df['question_id'].astype(str).str.strip() != \"\"]\n",
        "\n",
        "# Add a blank 'compliance' column\n",
        "df['compliance'] = \"\"\n",
        "\n",
        "# Keep only required columns: file, question_id, compliance\n",
        "df_filtered = df[['file', 'question_id', 'compliance']]\n",
        "\n",
        "# Save the filtered DataFrame\n",
        "output_path = \"/content/drive/MyDrive/project/CB/manual_marking.csv\"\n",
        "df_filtered.to_csv(output_path, index=False)\n",
        "\n",
        "# Display first 10 rows for confirmation\n",
        "display(df_filtered.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVOsV8nHvn5a"
      },
      "source": [
        "I created a simplified annotation file (manual_marking.csv) that only includes the file, question_id, and an empty compliance column. I removed the question_text because I’m already reading the full questions and answers directly from the original PDF files, so I don’t need to duplicate that in the CSV.\n",
        "\n",
        "This makes it easier for me to manually mark each application form one at a time. For example, when I open answers (1).pdf, I scroll through the questions, and for each one (e.g., A1.1, A1.2), I enter the compliance label — like \"Compliant\" or \"Non-Compliant\" — directly into the CSV under the right row.\n",
        "\n",
        "Later on, once I’ve annotated all the forms, I plan to merge these labels back into the full dataset using file and question_id as keys. That way, I’ll have the full data (question, answer, compliance) ready for model training or evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "574Z89z0ydlj"
      },
      "source": [
        "This script below updates the full question dataset (manual_null_compliance.csv) by merging in manually annotated compliance labels from a simpler marking file (manual_marking.csv). It uses the combination of file and question_id as a unique identifier to match and apply the correct compliance status. The result is saved as manual_nnotated_questions.csv, which now includes the updated compliance labels ready for analysis or model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkqGV-PluS9h"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load source of truth (with compliance column)\n",
        "ann_path = \"/content/drive/MyDrive/project/CB/manual_marking.csv\"\n",
        "mark_path = \"/content/drive/MyDrive/project/CB/manual_null_compliance.csv\"\n",
        "outpath= \"/content/drive/MyDrive/project/CB/manual_nnotated_questions.csv\"\n",
        "\n",
        "# Load the data\n",
        "df_ann = pd.read_csv(ann_path, dtype=str).fillna(\"\")\n",
        "df_mark = pd.read_csv(mark_path, dtype=str).fillna(\"\")\n",
        "\n",
        "# Build a mapping from (file, question_id) to compliance\n",
        "compliance_map = df_ann.set_index(['file', 'question_id'])['compliance'].to_dict()\n",
        "\n",
        "# Function to update compliance if match found\n",
        "def update_compliance(row):\n",
        "    key = (row['file'], row['question_id'])\n",
        "    return compliance_map.get(key, row.get('compliance', \"\"))\n",
        "\n",
        "df_mark['compliance'] = df_mark.apply(update_compliance, axis=1)\n",
        "\n",
        "# Save back to file\n",
        "df_mark.to_csv(outpath, index=False)\n",
        "print(f\"✅ Updated compliance labels in: {outpath}\")\n",
        "# Show the first 10 rows\n",
        "display(df_mark.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmhc_EotDszx"
      },
      "source": [
        "### **3. EXPLORATION OF MANUAL ANNOTATED DATA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTtxur24EGs8"
      },
      "source": [
        "### Number of application forms labelled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HFMO12NEGcN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "CSV_PATH = \"/content/drive/MyDrive/project/CB/manual_annotated_questions.csv\"\n",
        "df = pd.read_csv(CSV_PATH, dtype=str).fillna(\"\")\n",
        "\n",
        "# Count unique files\n",
        "unique_files = df[\"file\"].nunique()\n",
        "print(f\"Number of unique application forms (files): {unique_files}\")\n",
        "\n",
        "# Optionally, display the file names (first 10)\n",
        "print(\"\\nSample of unique file names:\")\n",
        "print(df[\"file\"].unique()[:10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAFDrbf3ER09"
      },
      "source": [
        "### check if are not duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0uS4irqE3Lj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "CSV_PATH = \"/content/drive/MyDrive/project/CB/manual_annotated_questions.csv\"\n",
        "df = pd.read_csv(CSV_PATH, dtype=str).fillna(\"\")\n",
        "\n",
        "# Count duplicates: same file + question_id\n",
        "dupes = df.duplicated(subset=[\"file\", \"question_id\"], keep=False)\n",
        "n_dupes = dupes.sum()\n",
        "\n",
        "if n_dupes == 0:\n",
        "    print(\"✅ No duplicate (file, question_id) pairs found. Each form has only one answer per question.\")\n",
        "else:\n",
        "    print(f\" Found {n_dupes} duplicate (file, question_id) rows. Here are the first few:\")\n",
        "    print(df.loc[dupes, [\"file\", \"question_id\", \"answer_text\"]].head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvr_16MgFnuc"
      },
      "outputs": [],
      "source": [
        "df_a1_1 = df[df[\"question_id\"] == \"A1.1\"]\n",
        "print(df_a1_1[[\"file\", \"answer_text\"]].head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rbu19pvBE6rq"
      },
      "outputs": [],
      "source": [
        "# Filter for A1.1 only\n",
        "df_a1_1 = df[df[\"question_id\"] == \"A1.1\"]\n",
        "\n",
        "# Check for duplicate answer_texts for A1.1 (across different files)\n",
        "dupe_answers = df_a1_1.duplicated(subset=[\"answer_text\"], keep=False)\n",
        "n_dupe_answers = dupe_answers.sum()\n",
        "\n",
        "if n_dupe_answers == 0:\n",
        "    print(\"✅ All A1.1 answers are unique across forms.\")\n",
        "else:\n",
        "    print(f\" Found {n_dupe_answers} A1.1 answers that are repeated across forms. Sample of first 10:\")\n",
        "    dupes_df = df_a1_1.loc[dupe_answers, [\"file\", \"answer_text\"]]\n",
        "    print(dupes_df.head(10))\n",
        "    print(\"\\nDistinct repeated answers in first 10 shown above:\")\n",
        "    print(dupes_df['answer_text'].head(10).tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VX_xWHRi4kls"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your data\n",
        "CSV_PATH = \"/content/drive/MyDrive/project/CB/manual_annotated_questions.csv\"\n",
        "df = pd.read_csv(CSV_PATH, dtype=str).fillna(\"\")\n",
        "\n",
        "unique_count = df[\"question_id\"].nunique()\n",
        "print(f\"Total unique question IDs: {unique_count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUf2Y4-P6AQ-"
      },
      "source": [
        "### Coverage of Question IDs Across All Forms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9Bahmm25cVh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your data\n",
        "CSV_PATH = \"/content/drive/MyDrive/project/CB/manual_annotated_questions.csv\"\n",
        "df = pd.read_csv(CSV_PATH, dtype=str).fillna(\"\")\n",
        "\n",
        "# Count how many distinct forms each question_id appears in\n",
        "qid_form_counts = df.groupby(\"question_id\")[\"file\"].nunique()\n",
        "\n",
        "# Get total number of forms\n",
        "total_forms = df[\"file\"].nunique()\n",
        "print(f\"Total forms: {total_forms}\")\n",
        "\n",
        "# Count question_ids that appear in ALL forms\n",
        "appear_in_all = (qid_form_counts == total_forms).sum()\n",
        "\n",
        "# Count question_ids that do NOT appear in all forms\n",
        "not_in_all = (qid_form_counts != total_forms).sum()\n",
        "\n",
        "print(f\"Question IDs appearing in ALL {total_forms} forms: {appear_in_all}\")\n",
        "print(f\"Question IDs NOT appearing in all forms: {not_in_all}\")\n",
        "\n",
        "# Optional: list them\n",
        "qids_in_all = qid_form_counts[qid_form_counts == total_forms].index.tolist()\n",
        "qids_not_in_all = qid_form_counts[qid_form_counts != total_forms].index.tolist()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5TDPNkx0zFD"
      },
      "source": [
        "### Occurrence counts per question_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XWAL1Q80x6z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your data\n",
        "CSV_PATH = \"/content/drive/MyDrive/project/CB/manual_annotated_questions.csv\"\n",
        "df = pd.read_csv(CSV_PATH, dtype=str).fillna(\"\")\n",
        "\n",
        "# Pivot table: question_id as rows, compliance labels as columns\n",
        "pivot = (\n",
        "    df.pivot_table(index=\"question_id\", columns=\"compliance\", values=\"file\", aggfunc=\"count\", fill_value=0)\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "# Add a total count column\n",
        "pivot[\"total_occurrences\"] = pivot.drop(columns=[\"question_id\"]).sum(axis=1)\n",
        "\n",
        "# Sort by total_occurrences ascending, then by question_id\n",
        "pivot_sorted = pivot.sort_values(by=[\"total_occurrences\", \"question_id\"], ascending=[True, True])\n",
        "\n",
        "# Show the 20 lowest\n",
        "display(pivot_sorted.head(22))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ku_O04mY2imN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "CSV_PATH = \"/content/drive/MyDrive/project/CB/manual_annotated_questions.csv\"\n",
        "df = pd.read_csv(CSV_PATH, dtype=str).fillna(\"\")\n",
        "\n",
        "# Pivot table\n",
        "pivot = (\n",
        "    df.pivot_table(index=\"question_id\", columns=\"compliance\", values=\"file\", aggfunc=\"count\", fill_value=0)\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "# Total occurrences\n",
        "pivot[\"total_occurrences\"] = pivot.drop(columns=[\"question_id\"]).sum(axis=1)\n",
        "\n",
        "# Sort and pick lowest 20\n",
        "pivot_sorted = pivot.sort_values(by=[\"total_occurrences\", \"question_id\"], ascending=[True, True]).head(22)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(pivot_sorted[\"question_id\"], pivot_sorted[\"total_occurrences\"], color=\"skyblue\")\n",
        "plt.xlabel(\"Total Occurrences\")\n",
        "plt.ylabel(\"Question ID\")\n",
        "plt.title(\"22 Question IDs with Lowest Total Occurrences\")\n",
        "plt.gca().invert_yaxis()  # Highest bar on top\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7re_X-LiK2V"
      },
      "source": [
        "### Analysis per question_types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xo5JoqSJZJKF"
      },
      "outputs": [],
      "source": [
        "yes_no_ids = {\n",
        "    \"A4.1\",\"A4.2\",\"A4.4\",\"A4.7\",\"A4.9\",\n",
        "    \"A5.2\",\"A5.3\",\"A5.8\",\"A5.9\",\n",
        "    \"A6.1\",\"A6.2\",\"A6.3\",\"A6.4\",\"A6.5\",\"A6.6\",\n",
        "    \"A7.2\",\"A7.8\",\"A7.9\",\"A7.13\",\"A7.16\",\"A7.17\",\n",
        "    \"A1.9\",\"A8.2\",\"A8.3\",\"A8.4\",\"A8.5\",\"A7.5\",\"A4.5.1\"\n",
        "}\n",
        "yes_no_info_only_ids = {\"A4.5\", \"A7.14\"} | {\"A2.1\",\"A3.1\",\"A4.8\",\"A4.11\",\"A5.4\"}  # Accept No + Non-scoring\n",
        "descriptive_info_only_ids = {\n",
        "    \"A1.1\",\"A1.2\",\"A1.3\",\"A1.4\",\"A1.5\",\"A1.6\",\"A1.7\",\n",
        "    \"A1.8\",\"A1.8.1\",\"A1.8.2\",\"A1.8.3\",\"A1.8.4\",\"A1.8.5\",\n",
        "    \"A1.10\",\n",
        "    \"A3.1\",\"A3.2\",\"A3.3\",\"A3.4\",\n",
        "    \"A4.11\",\n",
        "    \"A5.7\",\n",
        "    \"A6.4.1\",\"A6.5.1\",\"A1.11\",\"A7.15\"\n",
        "}\n",
        "multiple_choice_ids = {\"A4.3\", \"A5.5\", \"A8.1\"}\n",
        "\n",
        "semantic_ids = {\n",
        "    \"A2.2\",\"A2.10\",\"A2.3\",\"A2.4\",\"A2.4.1\",\"A2.5\",\"A2.6\",\"A2.7\",\"A2.7.1\",\"A2.8\",\"A2.9\",\n",
        "    \"A4.12\",\"A6.2.1\",\"A6.2.2\",\"A6.2.3\",\"A6.2.4\"\n",
        "}\n",
        "descriptive_ids = {\n",
        "    \"A4.1.1\", \"A4.10\", \"A4.2.1\", \"A4.6\", \"A5.1\", \"A5.10\", \"A5.6\",\n",
        "    \"A6.4.2\", \"A6.5.2\", \"A6.7\", \"A7.1\", \"A7.10\", \"A7.11\", \"A7.12\",\n",
        "    \"A7.3\", \"A7.4\", \"A7.5\", \"A7.6\", \"A7.7\"\n",
        "}\n",
        "\n",
        "def qtype(qid):\n",
        "    if qid in semantic_ids:\n",
        "        return \"List/Semantic\"\n",
        "    if qid in descriptive_ids:\n",
        "        return \"Descriptive\"\n",
        "    if qid in multiple_choice_ids:\n",
        "        return \"Multiple Choice\"\n",
        "    if qid in yes_no_ids:\n",
        "        return \"Yes/No\"\n",
        "    if qid in yes_no_info_only_ids:\n",
        "        return \"Yes/No (info only)\"\n",
        "    if qid in descriptive_info_only_ids:\n",
        "        return \"Descriptive (info only)\"\n",
        "    return \"Other\"\n",
        "\n",
        "\n",
        "df[\"qtype\"] = df[\"question_id\"].map(qtype)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "type_pivot = pd.pivot_table(\n",
        "    df, index=\"qtype\", columns=\"compliance\", aggfunc=\"size\", fill_value=0\n",
        ")\n",
        "\n",
        "type_pivot = type_pivot.loc[\n",
        "    [\"Yes/No\", \"Yes/No (info only)\", \"Descriptive\", \"Descriptive (info only)\", \"Multiple Choice\", \"List/Semantic\"]\n",
        "]  # order for plot\n",
        "\n",
        "type_pivot.plot(\n",
        "    kind=\"bar\", stacked=True, figsize=(12,7),\n",
        "    color=[\"forestgreen\",\"firebrick\",\"gold\",\"orange\"],\n",
        "    title=\"Label Distribution by Question Type\"\n",
        ")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xlabel(\"Question Type\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(type_pivot)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmiFYR_vhP88"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Prepare percentage pivot table ---\n",
        "type_counts = df.groupby('qtype')['compliance'].value_counts().unstack(fill_value=0)\n",
        "type_percent = type_counts.div(type_counts.sum(axis=1), axis=0) * 100\n",
        "\n",
        "# --- Stacked bar plot of proportions ---\n",
        "ax = type_percent.plot(\n",
        "    kind=\"bar\", stacked=True, figsize=(10,6),\n",
        "    color=[\"forestgreen\",\"firebrick\",\"gold\",\"orange\"],\n",
        "    title=\"Proportion of Labels by Question Type\"\n",
        ")\n",
        "plt.ylabel(\"Percentage (%)\")\n",
        "plt.xlabel(\"Question Type\")\n",
        "plt.legend(title=\"Compliance Label\", bbox_to_anchor=(1.05,1))\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-TQo_vQZI2l"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Get unique question_id → qtype mapping function as above\n",
        "qid2qtype = {qid: qtype(qid) for qid in df[\"question_id\"].unique()}\n",
        "\n",
        "# Count how many unique qids per qtype\n",
        "counts = Counter(qid2qtype.values())\n",
        "\n",
        "# Display as a simple table\n",
        "print(\"Typical number of questions per form (by type):\\n\")\n",
        "for t, c in counts.items():\n",
        "    print(f\"{t:<22} {c}\")\n",
        "\n",
        "# display as a bar plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.bar(list(counts.keys()), list(counts.values()), color=\"cornflowerblue\")\n",
        "plt.ylabel(\"Number of Questions per Form\")\n",
        "plt.xlabel(\"Question Type\")\n",
        "plt.title(\"Typical Distribution of Question Types per Application Form\")\n",
        "plt.xticks(rotation=20)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKPGszgGVj1Q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reload your current file (make sure it's saved!)\n",
        "CSV_PATH = \"/content/drive/MyDrive/project/CB/manual_annotated_questions.csv\"\n",
        "df = pd.read_csv(CSV_PATH, dtype=str).fillna(\"\")\n",
        "\n",
        "# Get value counts\n",
        "label_counts = df[\"compliance\"].value_counts(dropna=False)\n",
        "\n",
        "# Print counts for reference\n",
        "print(\"\\nLabel counts in 'compliance' column:\")\n",
        "print(label_counts)\n",
        "\n",
        "# Plot bar chart\n",
        "plt.figure(figsize=(7, 5))\n",
        "label_counts.plot(kind='bar', color=[\"mediumseagreen\", \"salmon\", \"gold\", \"steelblue\"])\n",
        "plt.title(\"Compliance Label Distribution\")\n",
        "plt.xlabel(\"Compliance Label\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks(rotation=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDqolCczTETs"
      },
      "source": [
        "### Summarisation of Application forms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mF_2OLUQTxDG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the manually annotated questions\n",
        "CSV_PATH = \"/content/drive/MyDrive/project/CB/manual_annotated_questions.csv\"\n",
        "df = pd.read_csv(CSV_PATH, dtype=str).fillna(\"\")\n",
        "\n",
        "# Group by 'file' and count labels\n",
        "summary = (\n",
        "    df.groupby('file')['compliance']\n",
        "    .value_counts()\n",
        "    .unstack(fill_value=0)\n",
        "    .reset_index()\n",
        ")\n",
        "for c in ['Compliant', 'Fail', 'Non-Compliant', 'More-information']:\n",
        "    if c not in summary.columns:\n",
        "        summary[c] = 0\n",
        "\n",
        "def get_form_type(row):\n",
        "    if row['Compliant'] > 0 and row['Fail'] == 0 and row['Non-Compliant'] == 0 and row['More-information'] == 0:\n",
        "        return \"All Compliant\"\n",
        "    elif row['Fail'] > 0:\n",
        "        return \"Contains Fail\"\n",
        "    elif row['Non-Compliant'] > 0 and row['Fail'] == 0:\n",
        "        return \"Has Non-Compliant\"\n",
        "    elif row['More-information'] == 1 and row['Fail'] == 0 and row['Non-Compliant'] == 0:\n",
        "        return \"Has Single More-info\"\n",
        "    elif row['More-information'] >= 2 and row['Fail'] == 0 and row['Non-Compliant'] == 0:\n",
        "        return \"2+ More-info (Count as Fail)\"\n",
        "    else:\n",
        "        return \"Other\"\n",
        "\n",
        "summary['FormStatus'] = summary.apply(get_form_type, axis=1)\n",
        "\n",
        "# Display counts as a bar plot and pie chart\n",
        "counts = summary['FormStatus'].value_counts()\n",
        "print(\"\\nApplication Form Status Counts:\\n\", counts)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "counts.plot.bar(color='skyblue')\n",
        "plt.ylabel(\"Number of Application Forms\")\n",
        "plt.title(\"Application Form Compliance Status Distribution\")\n",
        "plt.show()\n",
        "\n",
        "# Pie chart (optional)\n",
        "plt.figure(figsize=(7,7))\n",
        "counts.plot.pie(autopct='%1.1f%%', startangle=90, counterclock=False)\n",
        "plt.ylabel(\"\")\n",
        "plt.title(\"Application Form Status Breakdown\")\n",
        "plt.show()\n",
        "\n",
        "# Save file lists\n",
        "base = \"/content/drive/MyDrive/project/CB/\"\n",
        "types = [\n",
        "    (\"all_compliant_files.txt\", \"All Compliant\"),\n",
        "    (\"fail_files.txt\", \"Contains Fail\"),\n",
        "    (\"non_compliant_files.txt\", \"Has Non-Compliant\"),\n",
        "    (\"two_moreinfo_files.txt\", \"2+ More-info (Count as Fail)\"),\n",
        "    (\"single_moreinfo_files.txt\", \"Has Single More-info\")\n",
        "]\n",
        "for fname, formtype in types:\n",
        "    flist = summary.loc[summary.FormStatus == formtype, \"file\"]\n",
        "    with open(base+fname, \"w\") as f:\n",
        "        for fn in flist:\n",
        "            f.write(fn + \"\\n\")\n",
        "    print(f\"✅ Saved {len(flist)} files for '{formtype}' to {fname}\")\n",
        "\n",
        "# Optionally print the first few files in each category:\n",
        "for fname, formtype in types:\n",
        "    print(f\"\\n{formtype} (sample):\")\n",
        "    flist = summary.loc[summary.FormStatus == formtype, \"file\"].tolist()\n",
        "    print(flist[:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wGFPY50UqsJ"
      },
      "source": [
        "Most application forms (77%) are not fully compliant, as they contain at least one “Fail” response,indicating unmet essential requirements or missing answers. Additionally, 15% of forms have at least one “Non-Compliant” answer, reflecting serious issues that don’t qualify as outright fails. Only a small number of forms fall into the “More-information” category, showing that ambiguous or incomplete answers are relatively rare. Notably, no single form is fully compliant, highlighting  generally poor application quality as most have blank answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blpbF-yqstT3"
      },
      "source": [
        "### How many forms with atleast single blank answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4zwHlDksCTi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/project/CB/manual_annotated_questions.csv\", dtype=str).fillna(\"\")\n",
        "\n",
        "# Files with no blank answer_text\n",
        "files_no_blank = (\n",
        "    df.groupby(\"file\")[\"answer_text\"]\n",
        "    .apply(lambda answers: all(answers.str.strip() != \"\"))  # True if all answers are non-blank\n",
        ")\n",
        "\n",
        "# Filter to only those files\n",
        "files_no_blank = files_no_blank[files_no_blank].index.tolist()\n",
        "\n",
        "print(f\"Total files with no blank answers: {len(files_no_blank)}\")\n",
        "print(\"Sample files:\", files_no_blank[:10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb64W9bIDXwX"
      },
      "source": [
        "### Label counts for all questions of application forms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXWVBKDsMCss"
      },
      "source": [
        "The data is very imabalanced as majority of all data are compliant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwJUmOJqGDzJ"
      },
      "source": [
        "### Explore the labels counts per question_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6Qzw8neGJv1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your data\n",
        "CSV_PATH = \"/content/drive/MyDrive/project/CB/manual_annotated_questions.csv\"\n",
        "df = pd.read_csv(CSV_PATH, dtype=str).fillna(\"\")\n",
        "\n",
        "# Pivot table: question_id as rows, compliance labels as columns\n",
        "pivot = (\n",
        "    df.pivot_table(index=\"question_id\", columns=\"compliance\", values=\"file\", aggfunc=\"count\", fill_value=0)\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "# Show the first 20 rows\n",
        "print(pivot.head(20))\n",
        "\n",
        "# Save to CSV for further analysis\n",
        "pivot.to_csv(\"/content/drive/MyDrive/project/CB/label_pivot_per_question.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKjqU3X7F2e1"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KaJ1Vjjh3tg"
      },
      "source": [
        "### - Explore the labels distributions for descriptive question_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebD1voM3iP_H"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# --- Load your annotated data ---\n",
        "CSV_PATH = \"/content/drive/MyDrive/project/CB/manual_annotated_questions.csv\"\n",
        "df = pd.read_csv(CSV_PATH, dtype=str).fillna(\"\")\n",
        "\n",
        "# --- Define descriptive question IDs ---\n",
        "DESC_IDS = [\n",
        "    \"A4.1.1\",\"A4.10\",\"A4.2.1\",\"A4.6\",\"A5.1\",\"A5.10\",\"A5.6\",\n",
        "    \"A6.4.2\",\"A6.5.2\",\"A6.7\",\"A7.1\",\"A7.10\",\"A7.11\",\"A7.12\",\n",
        "    \"A7.3\",\"A7.4\",\"A7.5\",\"A7.6\",\"A7.7\"\n",
        "]\n",
        "\n",
        "# --- Filter for descriptive questions only ---\n",
        "df_desc = df[df[\"question_id\"].isin(DESC_IDS)]\n",
        "\n",
        "# --- Pivot table: label distribution per question_id ---\n",
        "pivot_desc = (\n",
        "    df_desc.pivot_table(index=\"question_id\",\n",
        "                        columns=\"compliance\",\n",
        "                        values=\"file\",\n",
        "                        aggfunc=\"count\",\n",
        "                        fill_value=0)\n",
        "           .reset_index()\n",
        ")\n",
        "\n",
        "# --- Standardize column order (Compliant, Non-Compliant) ---\n",
        "for col in [\"Compliant\", \"Non-Compliant\"]:\n",
        "    if col not in pivot_desc.columns:\n",
        "        pivot_desc[col] = 0\n",
        "pivot_desc = pivot_desc[[\"question_id\", \"Compliant\", \"Non-Compliant\"]]\n",
        "\n",
        "# --- Display in requested format ---\n",
        "print(\"Label count per question_id:\")\n",
        "print(pivot_desc.to_string(index=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc9FK22DjtQQ"
      },
      "source": [
        "**Explanations:**These data will need to be balanced as will be the one to train the LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBb4y9IMVjyb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the pivoted data\n",
        "pivot = pd.read_csv(\"/content/drive/MyDrive/project/CB/label_pivot_per_question.csv\")\n",
        "\n",
        "# Set the question_id as index for plotting\n",
        "pivot = pivot.set_index('question_id')\n",
        "\n",
        "# Plot stacked bar\n",
        "pivot.plot(\n",
        "    kind='bar',\n",
        "    stacked=True,\n",
        "    figsize=(18, 7),\n",
        "    colormap='tab20'\n",
        ")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Compliance Label Distribution per Question\")\n",
        "plt.legend(title='Label', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhdeWkbQjmLg"
      },
      "source": [
        "### Heatmap: Which Questions Most Often Trigger MIR/Fail/Non-Compliant\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGHz-kqIVjIr"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "sns.heatmap(pivot, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Heatmap of Compliance Labels per Question\")\n",
        "plt.ylabel(\"Question ID\")\n",
        "plt.xlabel(\"Compliance Label\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8R1M8WrQbyt"
      },
      "source": [
        "*Overall Pattern: Mostly Compliant*\n",
        "\n",
        "\n",
        "The majority of applicants answered most questions in a compliant way. This is clearly shown by the strong blue coloring in the “Compliant” column of the heatmap and the dominant blue segments in the barplots. This trend is especially noticeable in the first group of questions (such as A1.1 to A1.10), which are likely designed for information-gathering rather than strict assessment. As a result, almost any answer is considered compliant, except when left blank. This pattern also indicates that most applicants remembered to fill in these informational sections. These early questions are typically simple or relate to widely implemented controls, making them easier to answer correctly.\n",
        "\n",
        "\n",
        "\n",
        "*“Fail” and “More-Information” Hotspots*\n",
        "\n",
        "A smaller group of questions stands out with much darker shading in the “Fail” and “More-information” columns. This indicates that many applicants either fail these questions or need to provide more detail. These problematic questions include A2.4.1, A2.5, A6.7, A6.2.4, A2.8, A2.7.1, A6.2.2, and A6.2.3. The A6.2.x questions are especially prone to generating “More-information” outcomes—likely because they ask for detailed software inventory, patching info, and version details, which applicants often skip or answer incompletely. These areas are the main pain points in the process.\n",
        "\n",
        "\n",
        "\n",
        "*Non-Compliant Answers:*\n",
        "\n",
        "Rare but Concerning\n",
        "Although “Non-Compliant” answers are less common overall, a few specific questions show higher non-compliance rates. These include A4.2.1, A4.1.1, A5.1, A6.5.2, A7.4, A7.6, and A7.7. These questions are typically about critical security controls like multi-factor authentication, admin account rules, or patching policies. When applicants say “No” or admit to missing controls, the outcome is marked as Non-Compliant. These dark patches in the heatmap reflect real security or policy gaps in some organizations.\n",
        "\n",
        "\n",
        "\n",
        "*Uneven Question Distribution*\n",
        "\n",
        "Some questions show very little failure or variation, with minimal red, pink, or light blue in the charts. These are likely robust, clear, and easy-to-answer questions. In contrast, others—especially from A2.x, A6.x, and A7.x groups—have more diverse outcomes. This may be due to unclear guidance, technical complexity, or variability in organizational practices. The variation suggests that certain questions may need rewording or better examples to support applicants.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*Lower Response Counts for Some Questions*\n",
        "\n",
        "A few bars in the plots are shorter than others, indicating that some questions were asked only in specific cases (e.g., conditional logic based on earlier answers). These shorter bars likely reflect optional sections or scoped logic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYkUfQP4jhJK"
      },
      "source": [
        "### Heatmap by Proportionality: Which Questions Most Often Trigger MIR/Fail/Non-Compliant\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQppBssu-hJ4"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# --- Pivot by question_id ---\n",
        "question_pivot = pd.pivot_table(\n",
        "    df, index=\"question_id\", columns=\"compliance\", aggfunc=\"size\", fill_value=0\n",
        ")\n",
        "\n",
        "# --- (Optional) Select only relevant labels for heatmap clarity ---\n",
        "labels_of_interest = [\"Fail\", \"More-information\", \"Non-Compliant\"]\n",
        "heatmap_data = question_pivot[labels_of_interest]\n",
        "\n",
        "# --- Normalize to percent per question_id (optional) ---\n",
        "heatmap_percent = heatmap_data.div(question_pivot.sum(axis=1), axis=0) * 100\n",
        "\n",
        "# --- Plot heatmap ---\n",
        "plt.figure(figsize=(12, 12))\n",
        "sns.heatmap(\n",
        "    heatmap_percent, annot=True, fmt=\".1f\", cmap=\"Reds\",\n",
        "    cbar_kws={\"label\": \"Percent of Responses\"}\n",
        ")\n",
        "plt.title(\"Percent of Fail / MIR / Non-Compliant per Question ID\")\n",
        "plt.ylabel(\"Question ID\")\n",
        "plt.xlabel(\"Compliance Label\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHLWYPT9iwGz"
      },
      "source": [
        "The heatmap reveals that most questions are “low risk,” with very pale cells indicating that nearly all responses were marked as Compliant—especially for question IDs like A1.1, A1.2, A1.6, and A5.3, which show 0% in any risk category. However, there are several high-risk questions where applicants consistently struggle. For example, A2.4.1 has a almost 41% Fail rate making it major pain points. Questions like A2.5, A2.7.1, and A6.7 also show moderate fail rates (11–17%). Questions such as A6.2.4 (~ 60% “More-information”) and A6.2.2/3 (~ 30%+) indicate widespread confusion or lack of detail. A standout issue is A4.12, which is 100% Non-Compliant—possibly due to a question logic flaw. Other questions with moderate Non-Compliant rates (under 20%) include several from sections A4, A5, A6, and A7, reflecting ongoing issues with process adherence or policy enforcement in those areas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqJegppnImxN"
      },
      "source": [
        "### Sort Questions by \"Fail\", \"More-information\", or \"Non-Compliant\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOWc7FGQ-hVX"
      },
      "outputs": [],
      "source": [
        "top_fails         = pivot.sort_values(\"Fail\", ascending=False)[[\"Fail\"]].head(10)\n",
        "top_more_info     = pivot.sort_values(\"More-information\", ascending=False)[[\"More-information\"]].head(10)\n",
        "top_non_compliant = pivot.sort_values(\"Non-Compliant\", ascending=False)[[\"Non-Compliant\"]].head(20)\n",
        "\n",
        "print(\"Top 10 by Fail:\\n\", top_fails)\n",
        "print(\"\\nTop 10 by More-information:\\n\", top_more_info)\n",
        "print(\"\\nTop 10 by Non-Compliant:\\n\", top_non_compliant)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RQJ_o-lJo2w"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "CSV_PATH = \"/content/drive/MyDrive/project/CB/manual_annotated_questions.csv\"\n",
        "df = pd.read_csv(CSV_PATH, dtype=str).fillna(\"\")\n",
        "\n",
        "# 1. Filter all rows with \"Fail\" label\n",
        "fails = df[df[\"compliance\"].str.strip().str.lower() == \"fail\"].copy()\n",
        "\n",
        "# 2. Define what counts as 'blank/n/a' (tweak as needed)\n",
        "blank_patterns = {\"\", \"n/a\", \"na\", \"none\", \"not applicable\", \".\", \"n.a.\"}\n",
        "\n",
        "# Helper to check if answer_text is blank/n/a/none\n",
        "def is_blank(text):\n",
        "    return str(text).strip().lower() in blank_patterns\n",
        "\n",
        "fails[\"is_blank\"] = fails[\"answer_text\"].apply(is_blank)\n",
        "\n",
        "# 3. Count blank/n/a vs. not blank\n",
        "n_blank = fails[\"is_blank\"].sum()\n",
        "n_nonblank = (~fails[\"is_blank\"]).sum()\n",
        "\n",
        "print(f\"Total 'Fail' labels: {len(fails)}\")\n",
        "print(f\"Blank/n.a./none Fail answers: {n_blank}\")\n",
        "print(f\"Sentential Fail answers: {n_nonblank}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtC6h8jwKqkD"
      },
      "outputs": [],
      "source": [
        "# Group by question_id and count \"Fail\" for each question\n",
        "pivot_sorted_fail = (\n",
        "    df[df[\"compliance\"].str.strip().str.lower() == \"fail\"]\n",
        "    .groupby(\"question_id\")\n",
        "    .size()\n",
        "    .sort_values(ascending=False)\n",
        "    .to_frame(name=\"Fail\")\n",
        ")\n",
        "\n",
        "# Now plot the top 10 questions by 'Fail' count\n",
        "pivot_sorted_fail.head(10)[\"Fail\"].plot.bar(\n",
        "    figsize=(10, 5),\n",
        "    color='red',\n",
        "    title=\"Top 10 Questions by 'Fail' Count (including blanks)\"\n",
        ")\n",
        "plt.ylabel(\"Number of 'Fail' labels\")\n",
        "plt.xlabel(\"Question ID\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBvjxbzU-Wh2"
      },
      "source": [
        "- **Top “Fail” Questions**\n",
        "\n",
        "The most common failures occur in the A2 section, which focuses on device and asset inventory, including questions like A2.4.1, A2.5, A2.7.1, A2.6, A2.4, and A2.8. These questions typically ask for detailed listings of hardware (servers, laptops), operating systems, device counts, and network scope. Applications fail here mostly due to blank fields, vague or incomplete asset lists, unsupported operating systems, or failure to provide specific technical details.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcNUbUe8J6Yd"
      },
      "outputs": [],
      "source": [
        "# Filter 'Fail' rows where answer_text is NOT blank/n/a\n",
        "sentential_fails = fails[~fails[\"is_blank\"]]\n",
        "\n",
        "# Count per question_id\n",
        "sentential_fail_counts = sentential_fails[\"question_id\"].value_counts().reset_index()\n",
        "sentential_fail_counts.columns = [\"question_id\", \"sentential_fail_count\"]\n",
        "\n",
        "print(\"Sentential 'Fail' answer counts per question_id:\")\n",
        "print(sentential_fail_counts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOLqUJ2FLCZq"
      },
      "outputs": [],
      "source": [
        "# 1. Filter 'Fail' rows where answer_text is NOT blank/n/a\n",
        "sentential_fails = fails[~fails[\"is_blank\"]]\n",
        "\n",
        "# 2. Count per question_id\n",
        "sentential_fail_counts = sentential_fails[\"question_id\"].value_counts().reset_index()\n",
        "sentential_fail_counts.columns = [\"question_id\", \"sentential_fail_count\"]\n",
        "\n",
        "# 3. Sort and plot\n",
        "sentential_fail_counts_sorted = sentential_fail_counts.sort_values(\"sentential_fail_count\", ascending=False)\n",
        "\n",
        "sentential_fail_counts_sorted.head(10).set_index(\"question_id\")[\"sentential_fail_count\"].plot.bar(\n",
        "    figsize=(10,5),\n",
        "    color='red',\n",
        "    title=\"Top 10 Questions by Sentential 'Fail' Count\"\n",
        ")\n",
        "plt.ylabel(\"Number of 'Fail' labels (sentential)\")\n",
        "plt.xlabel(\"Question ID\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hGne_fE-IFl"
      },
      "source": [
        "These obseravtions are due to :\n",
        "\n",
        "- The items A2.4 – A2.8 belong to the “Scope of Assessment” section. Under the marking scheme, listing any unsupported endpoint or network device (e.g., Windows 7 PCs, iOS 12 phones, EoL firewalls) results in an automatic Fail, irrespective of the rest of the submission.\n",
        "\n",
        "- Because of this zero-tolerance rule, the semantic-frame component of the pipeline must apply a strict auto-fail check for these questions: if the extracted inventory contains an OS or firmware whose vendor support has lapsed, the answer should be flagged Fail immediately, bypassing further NLP scoring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2olErhpc-hRq"
      },
      "outputs": [],
      "source": [
        "top_more_info.plot.bar(\n",
        "    figsize=(10,5), color='orange', legend=False,\n",
        "    title=\"Top 10 Questions by 'More-information' Count\"\n",
        ")\n",
        "plt.ylabel(\"Number of 'More-information' labels\")\n",
        "plt.xlabel(\"Question ID\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIf0UZqa-jcV"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "- **Top “More-Information” (MIR) Questions**\n",
        "\n",
        "\n",
        "These questions typically ask for long-form or exhaustive lists. The A6.2.x group (e.g., A6.2.2, A6.2.3, A6.2.4) focuses on software applications, versions, and patching details, which are often difficult for applicants to address thoroughly. Similarly, A2.8, A2.10, and A2.3 request detailed enumerations of assets, software, or BYOD (Bring Your Own Device) controls. Applicants frequently receive MIRs because they submit partial answers, use ambiguous wording, or omit specific information like software versions or the scope of control.\n",
        "\n",
        "Summary: MIRs are common when the question demands detailed or technical lists, and the applicant fails to provide full, precise, and unambiguous information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vx1UjFvF-hN8"
      },
      "outputs": [],
      "source": [
        "top_non_compliant.plot.bar(\n",
        "    figsize=(12,5), color='purple', legend=False,\n",
        "    title=\"Top 20 Questions by 'Non-Compliant' Count\"\n",
        ")\n",
        "plt.ylabel(\"Number of 'Non-Compliant' labels\")\n",
        "plt.xlabel(\"Question ID\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyXoTGXcMnID"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUNAW16ENP3J"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- **Top “Non-Compliant” Questions**\n",
        "\n",
        "\n",
        "Non-compliance is most frequently seen in areas related to security controls and enforcement of policies. Questions like A4.2.1 and A4.1.1 relate to controls such as multi-factor authentication (MFA) and account protection. A5.1 and A5.10 cover password policies and account management, while A6.x and A7.x relate to patching processes and administrative rights. These are marked non-compliant when the applicant admits to missing controls, describes non-compliant practices, or explicitly states “No” when asked about a required control.\n",
        "\n",
        "Summary: Non-compliant outcomes typically occur when a critical security measure is missing, not enforced, or inadequately described, revealing a clear gap in security or governance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGiNZve6lD4B"
      },
      "source": [
        "### **Analysis of Scoring-Descriptive and List-type questions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cye0Q0lxlT9q"
      },
      "source": [
        "- ### Average number of words per question_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQzeyv7u-hBf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# File path\n",
        "CSV_PATH = \"/content/drive/MyDrive/project/CB/manual_annotated_questions.csv\"\n",
        "df = pd.read_csv(CSV_PATH, dtype=str).fillna(\"\")\n",
        "\n",
        "# Sets for question IDs\n",
        "semantic_ids = {\n",
        "    \"A2.2\",\"A2.10\",\"A2.3\",\"A2.4\",\"A2.4.1\",\"A2.5\",\"A2.6\",\"A2.7\",\"A2.7.1\",\"A2.8\",\"A2.9\",\n",
        "    \"A4.12\",\"A6.2.1\",\"A6.2.2\",\"A6.2.3\",\"A6.2.4\"\n",
        "}\n",
        "descriptive_ids = {\n",
        "    \"A4.1.1\", \"A4.10\", \"A4.2.1\", \"A4.6\", \"A5.1\", \"A5.10\", \"A5.6\",\n",
        "    \"A6.4.2\", \"A6.5.2\", \"A6.7\", \"A7.1\", \"A7.10\", \"A7.11\", \"A7.12\",\n",
        "    \"A7.3\", \"A7.4\", \"A7.5\", \"A7.6\", \"A7.7\"\n",
        "}\n",
        "all_ids = semantic_ids | descriptive_ids\n",
        "\n",
        "# Filter to relevant rows\n",
        "filtered = df[df['question_id'].isin(all_ids)].copy()\n",
        "\n",
        "# Function to count words\n",
        "def word_count(text):\n",
        "    return len(str(text).strip().split())\n",
        "\n",
        "filtered['word_count'] = filtered['answer_text'].apply(word_count)\n",
        "\n",
        "# Group by question_id, aggregate stats\n",
        "word_stats = (\n",
        "    filtered.groupby(\"question_id\")['word_count']\n",
        "    .agg(['mean', 'median', 'min', 'max', 'count'])\n",
        "    .sort_values('mean', ascending=False)\n",
        ")\n",
        "print(word_stats.head(10))  # show sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWe2R2pS-g5h"
      },
      "outputs": [],
      "source": [
        "def qtype(qid):\n",
        "    if qid in semantic_ids:\n",
        "        return \"Semantic\"\n",
        "    elif qid in descriptive_ids:\n",
        "        return \"Descriptive\"\n",
        "    else:\n",
        "        return \"Other\"\n",
        "\n",
        "word_stats['type'] = word_stats.index.map(qtype)\n",
        "\n",
        "# You can then plot by type if you wish\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "sns.barplot(\n",
        "    data=word_stats.reset_index(),\n",
        "    x=\"question_id\", y=\"mean\", hue=\"type\", dodge=False, palette=\"Set1\"\n",
        ")\n",
        "plt.title(\"Average Answer Length by Question ID and Type\")\n",
        "plt.ylabel(\"Average Word Count\")\n",
        "plt.xlabel(\"Question ID\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title=\"Type\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlNvvhlKnzyS"
      },
      "source": [
        "The analysis of average answer lengths shows that descriptive questions consistently receive the longest responses. The top 10 question IDs by word count are all descriptive (e.g., A7.5, A4.2.1, A5.1), with averages ranging from 46 to 58 words and some responses exceeding 300–400 words. These questions typically ask applicants to describe processes or justify security controls, which leads to longer, more detailed answers. However, minimum word counts of zero reveal that some forms leave these answers blank or include only short placeholders like “N/A.” The distributions are skewed (mean greater than median), suggesting a few applicants write excessively long answers, possibly pasting in policy documents. These descriptive questions are high-effort both to complete and to assess, often becoming subjective judgment points. In contrast, semantic or list-type questions tend to produce shorter, more uniform answers, usually consisting of names or simple factual inputs. Overall, the length of answers reflects the complexity and effort required, with implications for both applicant clarity and assessor workload."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwYnVkJ0VosS"
      },
      "source": [
        "### - Distribution of words lengths across labels for both semantic and Descriptive questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QrUdw5aonWk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load data\n",
        "CSV_PATH = \"/content/drive/MyDrive/project/CB/manual_annotated_questions.csv\"\n",
        "df = pd.read_csv(CSV_PATH, dtype=str).fillna(\"\")\n",
        "\n",
        "# Define your ID sets\n",
        "semantic_ids = {\n",
        "    \"A2.2\",\"A2.10\",\"A2.3\",\"A2.4\",\"A2.4.1\",\"A2.5\",\"A2.6\",\"A2.7\",\"A2.7.1\",\"A2.8\",\"A2.9\",\n",
        "    \"A4.12\",\"A6.2.1\",\"A6.2.2\",\"A6.2.3\",\"A6.2.4\"\n",
        "}\n",
        "descriptive_ids = {\n",
        "    \"A4.1.1\", \"A4.10\", \"A4.2.1\", \"A4.6\", \"A5.1\", \"A5.10\", \"A5.6\",\n",
        "    \"A6.4.2\", \"A6.5.2\", \"A6.7\", \"A7.1\", \"A7.10\", \"A7.11\", \"A7.12\",\n",
        "    \"A7.3\", \"A7.4\", \"A7.5\", \"A7.6\", \"A7.7\"\n",
        "}\n",
        "\n",
        "# Add word count column\n",
        "df['word_count'] = df['answer_text'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "# Subset for each type\n",
        "df_semantic = df[df['question_id'].isin(semantic_ids)]\n",
        "df_desc = df[df['question_id'].isin(descriptive_ids)]\n",
        "\n",
        "# Plot for Semantic IDs\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=df_semantic, x='compliance', y='word_count', order=['Compliant', 'Fail', 'More-information', 'Non-Compliant'])\n",
        "plt.title('Word Count vs Compliance Label (Semantic Questions)')\n",
        "plt.show()\n",
        "\n",
        "# Plot for Descriptive IDs\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=df_desc, x='compliance', y='word_count', order=['Compliant', 'Fail', 'More-information', 'Non-Compliant'])\n",
        "plt.title('Word Count vs Compliance Label (Descriptive Questions)')\n",
        "plt.show()\n",
        "\n",
        "# Optionally: Print stats per label\n",
        "print(\"Semantic questions, word count by label:\")\n",
        "print(df_semantic.groupby(\"compliance\")[\"word_count\"].describe())\n",
        "print(\"\\nDescriptive questions, word count by label:\")\n",
        "print(df_desc.groupby(\"compliance\")[\"word_count\"].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTUBgVv-VSQd"
      },
      "source": [
        "Semantic-list answers\n",
        "\n",
        "---\n",
        "Semantic-list answers show substantial overlap in word\n",
        "count between Compliant and Non-Compliant labels, with both typically falling within a 4–15 word range. This means length alone doesn’t distinguish them well. Fail responses, however, cluster around 0–2 words, indicating they are mostly blank or token replies. Notably, Compliant outliers can be very long (often detailed device lists), while Non-Compliant answers rarely exceed 30 words. All categories except Fail are right-skewed, showing occasional long answers. In short, answer length is a reliable signal for identifying Fail responses, but not enough to differentiate Compliant from Non-Compliant—more nuanced content features are needed for that.\n",
        "\n",
        "Descriptive-answers\n",
        "\n",
        "---\n",
        "\n",
        "In descriptive (narrative) responses, answer length is a strong indicator of quality. Fail answers are nearly all blank or one-word entries. Compliant responses are significantly longer than Non-Compliant ones, often twice as long, and only Compliant answers contain very lengthy outliers (over 100 words). This suggests that assessors value detailed explanations but penalize vague or incomplete ones rather than length alone. All non-Fail groups are right-skewed, especially Compliant, showing that elaboration is encouraged but not required. Practically, a length of ~30 words often signals compliance, while answers under 10 words may need further review or be flagged as needing more information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQO86BEEsZo1"
      },
      "source": [
        "- ### Correlation Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59L8A0tjonHn"
      },
      "outputs": [],
      "source": [
        "# Make an explicit copy to avoid SettingWithCopyWarning\n",
        "df_desc = df[df['question_id'].isin(descriptive_ids)].copy()\n",
        "\n",
        "# For descriptive questions, make compliance binary (1=Compliant, 0=other)\n",
        "df_desc['is_compliant'] = (df_desc['compliance'] == \"Compliant\").astype(int)\n",
        "\n",
        "corr = df_desc[['word_count', 'is_compliant']].corr(method='spearman')\n",
        "print(\"Spearman correlation between word count and Compliant (Descriptive):\\n\", corr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdy5qTbMsQjR"
      },
      "source": [
        "- ### Regression/Trendline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fFwNED5onD0"
      },
      "outputs": [],
      "source": [
        "sns.lmplot(\n",
        "    x='word_count', y='is_compliant', data=df_desc,\n",
        "    logistic=True, y_jitter=.03, height=5, aspect=1.5\n",
        ")\n",
        "plt.title(\"Probability of Compliance vs. Word Count (Descriptive)\")\n",
        "plt.xlabel(\"Word Count\")\n",
        "plt.ylabel(\"Probability (Compliant)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVqj1Qx0trAP"
      },
      "source": [
        "The analysis reveals a clear relationship between answer length and compliance. Compliant answers tend to be significantly longer, with many exceeding 40–50 words and some going beyond 100, as shown by the higher medians and quartiles in the boxplot. In contrast, “Fail” answers are mostly very short—often under 10 words—likely due to blanks or minimal entries like “n/a.” “Non-Compliant” answers are generally short as well but slightly longer than “Fail,” while “More-information” responses fall somewhere in between.\n",
        "\n",
        "\n",
        "A scatter plot of compliance probability versus word count shows a steep upward trend: short answers (<20 words) are rarely compliant, but as length increases past 30–40 words, the likelihood of compliance rises rapidly, reaching over 80% and nearing certainty at 60–80 words. The relationship follows a non-linear curve, with a sharp rise followed by a plateau.\n",
        "\n",
        "\n",
        "Statistically, a Spearman’s rho of 0.43421 indicates a moderate positive correlation—longer answers are more likely to be compliant, though exceptions exist."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnvK06u4wKWf"
      },
      "source": [
        "### **Lexical & Linguistic Patterns**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SyKUca5_5xh"
      },
      "source": [
        "### - Top Bigrams/Trigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMDDTl6jom_0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "CSV_PATH = \"/content/drive/MyDrive/project/CB/manual_annotated_questions.csv\"\n",
        "df = pd.read_csv(CSV_PATH, dtype=str).fillna(\"\")\n",
        "\n",
        "# IDs\n",
        "descriptive_ids = {\n",
        "    \"A4.1.1\", \"A4.10\", \"A4.2.1\", \"A4.6\", \"A5.1\", \"A5.10\", \"A5.6\",\n",
        "    \"A6.4.2\", \"A6.5.2\", \"A6.7\", \"A7.1\", \"A7.10\", \"A7.11\", \"A7.12\",\n",
        "    \"A7.3\", \"A7.4\", \"A7.5\", \"A7.6\", \"A7.7\"\n",
        "}\n",
        "semantic_ids = {\n",
        "    \"A2.2\", \"A2.10\", \"A2.3\", \"A2.4\", \"A2.4.1\", \"A2.5\", \"A2.6\", \"A2.7\", \"A2.7.1\", \"A2.8\", \"A2.9\",\n",
        "    \"A4.12\", \"A6.2.1\", \"A6.2.2\", \"A6.2.3\", \"A6.2.4\"\n",
        "}\n",
        "\n",
        "# Filter relevant rows\n",
        "desc_sem_df = df[df['question_id'].isin(descriptive_ids | semantic_ids)].copy()\n",
        "desc_sem_df['qtype'] = desc_sem_df['question_id'].map(\n",
        "    lambda qid: \"Descriptive\" if qid in descriptive_ids else \"List/Semantic\"\n",
        ")\n",
        "\n",
        "for qtype in [\"Descriptive\", \"List/Semantic\"]:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "    for i, label in enumerate([\"Compliant\", \"Non-Compliant\"]):\n",
        "        mask = (desc_sem_df['qtype'] == qtype) & (desc_sem_df['compliance'] == label)\n",
        "        texts = desc_sem_df.loc[mask, 'answer_text'].dropna().tolist()\n",
        "        if not texts:\n",
        "            axes[i].set_title(f\"{qtype}: {label} (No Data)\")\n",
        "            axes[i].axis(\"off\")\n",
        "            continue\n",
        "        vect = CountVectorizer(ngram_range=(2, 3), stop_words='english', min_df=1, max_features=15)\n",
        "        X = vect.fit_transform(texts)\n",
        "        if X.shape[1] == 0:\n",
        "            axes[i].set_title(f\"{qtype}: {label} (No ngrams found)\")\n",
        "            axes[i].axis(\"off\")\n",
        "            continue\n",
        "        ngram_freq = dict(zip(vect.get_feature_names_out(), X.sum(axis=0).A1))\n",
        "        ngram_items = sorted(ngram_freq.items(), key=lambda x: -x[1])\n",
        "        axes[i].barh(\n",
        "            [k for k, v in ngram_items],\n",
        "            [v for k, v in ngram_items],\n",
        "            color=\"forestgreen\" if label == \"Compliant\" else \"darkred\"\n",
        "        )\n",
        "        axes[i].set_title(f\"{qtype}: {label}\")\n",
        "    plt.suptitle(f\"Top Bigrams/Trigrams for {qtype} Answers\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0BHp9yX5FFN"
      },
      "source": [
        "**1st Takeaway:**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        " - The better answers use proper security terms. Vague or confusing answers are more likely to be non-compliant.\n",
        "\n",
        "When people write good (compliant) descriptive answers, they often use clear and specific phrases like “user account,” “password policy,” or “admin accounts.” These show they understand what the question is asking.\n",
        "\n",
        "In bad (non-compliant) answers, we see vague or odd phrases like “day day” or “field question,” which might mean someone copied text or didn’t understand the question.\n",
        "\n",
        "\n",
        "\n",
        "**2nd Takeaway:**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        " - Listing exact tools or people helps. Jumbled or vague lists hurt the score.\n",
        "\n",
        "In compliant list answers, people often name specific software or roles like “Windows 11,” “Microsoft 365,” or “responsible person.” This is helpful and shows they know what they’re talking about.\n",
        "\n",
        "Non-compliant answers include strange or messy phrases like “18 24090 11” or “office premises disabled,” which could be typos or unclear references.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNySMCL0y41D"
      },
      "source": [
        "#### - Vague Language Detection (Heatmap by question_id & Label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkC5k6o9om5C"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# Define basic hedging terms (expand as needed)\n",
        "vague_terms = r\"\\b(possible|generally|likely|sometimes|usually|typically|often|maybe|as needed|if necessary|where appropriate|as appropriate|in most cases|can|could|may|might|should|mainly|almost|somewhat|relatively|rarely|commonly|depends)\\b\"\n",
        "\n",
        "desc_sem_df['vague_flag'] = desc_sem_df['answer_text'].str.contains(vague_terms, case=False, regex=True, na=False)\n",
        "\n",
        "# Pivot: % of vague answers per qid and label\n",
        "heat = pd.pivot_table(\n",
        "    desc_sem_df, index='question_id', columns='compliance',\n",
        "    values='vague_flag', aggfunc='mean', fill_value=0\n",
        ")\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.heatmap(heat*100, annot=True, fmt=\".1f\", cmap=\"YlOrRd\")\n",
        "plt.title(\"Percent of Answers with Vague Language per Question and Label\")\n",
        "plt.ylabel(\"Question ID\")\n",
        "plt.xlabel(\"Compliance Label\")\n",
        "plt.tight_layout(); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eudY9SfM6JaS"
      },
      "source": [
        "Key Observations:\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "A5.6 (Non-Compliant = 100 %) – every NC answer is vague.\n",
        "\n",
        "\n",
        "\n",
        "A4.2.1 (MIR ≈ 33 %) – a third of “More-information” answers are vague.\n",
        "\n",
        "\n",
        "\n",
        "A5.1 & A5.6 (Compliant ≈ 28–31 %) – even some “Compliant” responses are still vague here.\n",
        "\n",
        "\n",
        "\n",
        "A7.5 (Compliant ≈ 30 %) – admin-access question often answered vaguely.\n",
        "\n",
        "\n",
        "\n",
        "Virtually zero vagueness in most A2 (inventory) questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqAr_rIu4o5W"
      },
      "source": [
        "#### - Stopword Percentage by Question Type and Label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvXEuipRom2M"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "\n",
        "def stopword_percent(text):\n",
        "    tokens = [w for w in str(text).split() if w.isalpha()]\n",
        "    if not tokens: return 0\n",
        "    stopwords = [w for w in tokens if w.lower() in ENGLISH_STOP_WORDS]\n",
        "    return len(stopwords)/len(tokens)*100\n",
        "\n",
        "desc_sem_df['stopword_pct'] = desc_sem_df['answer_text'].apply(stopword_percent)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.boxplot(x='qtype', y='stopword_pct', hue='compliance', data=desc_sem_df, showfliers=False)\n",
        "plt.title(\"Stopword Percentage by Question Type and Label\")\n",
        "plt.ylabel(\"Stopword % of All Words\")\n",
        "plt.xlabel(\"Question Type\")\n",
        "plt.legend(bbox_to_anchor=(1.01, 1.02))\n",
        "plt.tight_layout(); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUG_0y0U7Wh6"
      },
      "source": [
        "The boxplot titled “Stopword Percentage by Question Type and Label” reveals clear differences in stopword usage between question types, but limited predictive power within labels. For List/Semantic questions, Compliant and More-Information answers show very low median stopword percentages (10–15%) with heavy overlap, confirming that stopword density does not distinguish these labels. Non-Compliant responses in this category have slightly higher medians (20%) and wider spread, indicating vaguer, more verbose answers, though overlap with other labels remains high. In contrast, Descriptive answers—both Compliant and Non-Compliant—exhibit much higher median stopword percentages (45–55%) due to natural sentence structure, with little separation between the labels. Overall, stopword percentage may help flag vague or poorly structured List responses, but it is not a reliable indicator for Descriptive answers or for separating More-Information from Compliant cases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meu1b7lL4Sxt"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vN3hpcOGhjL"
      },
      "source": [
        "### - Keyword and Bigram Frequency Analysis in Device and Asset Listing Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_sscblF-7B6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# === Load your annotated dataset ===\n",
        "CSV_PATH = \"/content/drive/MyDrive/project/CB/manual_annotated_questions.csv\"\n",
        "df = pd.read_csv(CSV_PATH, dtype=str).fillna(\"\")\n",
        "\n",
        "# === Semantic question IDs to analyze ===\n",
        "semantic_ids = [\n",
        "    \"A2.3\", \"A2.4\", \"A2.4.1\", \"A2.5\", \"A2.6\", \"A2.7\", \"A2.7.1\",\n",
        "    \"A2.8\", \"A2.9\", \"A4.12\", \"A6.2.1\", \"A6.2.2\", \"A6.2.3\", \"A6.2.4\"\n",
        "]\n",
        "\n",
        "# === Custom stopwords (expand if needed) ===\n",
        "custom_stopwords = [\n",
        "    \"the\", \"and\", \"of\", \"to\", \"a\", \"in\", \"for\", \"with\", \"on\", \"by\", \"is\", \"at\", \"as\", \"or\", \"it\", \"from\",\n",
        "    \"all\", \"not\", \"only\", \"be\", \"that\", \"are\", \"an\", \"this\", \"must\", \"should\", \"include\", \"within\"\n",
        "]\n",
        "\n",
        "# === Loop through each semantic question ID ===\n",
        "for qid in semantic_ids:\n",
        "    # Filter for answers to this question\n",
        "    texts = df[df['question_id'] == qid]['answer_text'].dropna().tolist()\n",
        "\n",
        "    if not texts:\n",
        "        continue  # Skip if no data\n",
        "\n",
        "    # Set up CountVectorizer to extract unigrams and bigrams\n",
        "    vect = CountVectorizer(\n",
        "        ngram_range=(1, 2),\n",
        "        stop_words=custom_stopwords,\n",
        "        max_features=50,\n",
        "        token_pattern=r'(?u)\\b[a-zA-Z0-9._+-]+\\b'\n",
        "    )\n",
        "\n",
        "    # Vectorize the text\n",
        "    X = vect.fit_transform(texts)\n",
        "    freqs = dict(zip(vect.get_feature_names_out(), X.sum(axis=0).A1))\n",
        "\n",
        "    if not freqs:\n",
        "        continue  # Skip if no valid terms\n",
        "\n",
        "    # === Generate and show word cloud ===\n",
        "    wc = WordCloud(\n",
        "        width=800,\n",
        "        height=400,\n",
        "        background_color=\"white\",\n",
        "        colormap=\"Dark2\"\n",
        "    ).generate_from_frequencies(freqs)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wc, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"{qid}: Key Mentions (Top Keywords/Bigrams)\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbPOM2fcCi4e"
      },
      "source": [
        "Most forms (A2.3) mention traditional devices like laptops, desktops, and servers, while tablets and virtual machines are rarely included. Windows 11 and 10 are the main operating systems in A2.4, with Windows 11 now leading, and macOS also present. In A2.4.1, thin clients are rarely used, and older or unsupported systems like Windows 7 appear very little. A2.5 shows that modern server OSs like Windows Server 2022, Ubuntu 22.04, and AWS EC2 are common, though some older on-prem servers still show up. For mobile devices (A2.6), iOS and Android are both popular, with newer versions dominating.\n",
        "\n",
        "\n",
        " A2.7 highlights the use of tools like Intune, JAMF, and WSUS for firmware updates and OS configuration, while manual updates often flag issues. In A2.7.1, most forms clearly name a responsible party like IT Manager or MSP. A2.8 shows that security settings often use Intune Baseline, Group Policy, or JAMF Pro, and A2.9 confirms that auto-updates are the norm. A4.12 reveals that firewall rules like “disabled on domain” are common but sometimes contradict other answers.\n",
        "\n",
        "\n",
        "  A6.2.1 indicates most applicants use MFA or 2FA, A6.2.2 confirms that default passwords are changed, and A6.2.3 shows that strong password policies (12+ characters and complexity) are widely applied. Finally, A6.2.4 reports that Microsoft 365 is the most commonly licensed app, followed by Google Workspace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygX6VwPeMRgv"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOmYHaXsJUWl"
      },
      "source": [
        "### - Distributional Analysis of Descriptive Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZw4USYHNrqM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load CSV file\n",
        "input_path = \"/content/drive/MyDrive/project/CB/manual_annotated_questions.csv\"\n",
        "df = pd.read_csv(input_path, dtype=str).fillna(\"\")\n",
        "\n",
        "# Define descriptive question IDs\n",
        "descriptive_ids = [\n",
        "    \"A4.1.1\", \"A4.10\", \"A4.2.1\", \"A4.6\", \"A5.1\", \"A5.10\", \"A5.6\",\n",
        "    \"A6.4.2\", \"A6.5.2\", \"A6.7\", \"A7.1\", \"A7.10\", \"A7.11\", \"A7.12\",\n",
        "    \"A7.3\", \"A7.4\", \"A7.5\", \"A7.6\", \"A7.7\"\n",
        "]\n",
        "\n",
        "desc_df = df[df[\"question_id\"].isin(descriptive_ids)].copy()\n",
        "desc_df = desc_df[desc_df[\"compliance\"].str.strip() != \"\"]\n",
        "\n",
        "# Label distribution per question_id\n",
        "label_counts = (\n",
        "    desc_df.groupby([\"question_id\", \"compliance\"])\n",
        "    .size()\n",
        "    .unstack(fill_value=0)\n",
        "    .sort_index()\n",
        ")\n",
        "\n",
        "print(\"\\nLabel distribution for each descriptive question ID:\")\n",
        "print(label_counts)\n",
        "\n",
        "# ---- Plot as before ----\n",
        "label_counts.plot(\n",
        "    kind='bar',\n",
        "    figsize=(15,6),\n",
        "    color=[\"forestgreen\", \"firebrick\", \"gold\", \"orange\"],\n",
        "    edgecolor='black'\n",
        ")\n",
        "plt.title(\"Compliance Label Distribution per Descriptive Question ID\")\n",
        "plt.ylabel(\"Number of Answers\")\n",
        "plt.xlabel(\"Question ID\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---- Extra: Analyze the Fails ----\n",
        "fail_mask = (desc_df[\"compliance\"].str.lower() == \"fail\")\n",
        "fails = desc_df[fail_mask].copy()\n",
        "\n",
        "# \"Blank-like\" answers: blank, \".\", \"n/a\", \"na\", \"none\", \"not applicable\" (case-insensitive)\n",
        "blank_vals = {\"\", \".\", \"n/a\", \"na\", \"none\", \"not applicable\"}\n",
        "fails[\"is_blank\"] = fails[\"answer_text\"].str.strip().str.lower().isin(blank_vals)\n",
        "\n",
        "fail_summary = (\n",
        "    fails.groupby([\"question_id\", \"is_blank\"])\n",
        "    .size()\n",
        "    .unstack(fill_value=0)\n",
        "    .rename(columns={True: \"Blank/NA/.\", False: \"Sentential\"})\n",
        ")\n",
        "\n",
        "print(\"\\n[Fail answers per question_id: blank vs sentential]\")\n",
        "print(fail_summary)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uBVxbPjfLfF"
      },
      "source": [
        "The current label distribution across the descriptive questions is highly imbalanced, with most answers marked as Compliant, very few as Non-Compliant, and the majority of Fail cases resulting from blank or missing responses rather than meaningful text. This means that if these responses are used to train LLMs, the models will mostly learn to recognize Compliant patterns and fail to generalize to Non-Compliant scenarios, especially since the only signals for Fail are empty entries. To build a reliable LLM, it's important to manually create realistic Non-Compliant answers and balance the dataset before training, ensuring the model can distinguish between all labels effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGQAxtKvJc8C"
      },
      "source": [
        "### **Analysis of cross-depndence among question_ids**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZC7qcirKKIc"
      },
      "source": [
        "### - A6.2.x → A6.2/A6.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jL6vJcaomyt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/project/CB/manual_annotated_questions.csv\", dtype=str).fillna(\"\")\n",
        "\n",
        "violations = []\n",
        "\n",
        "for file, sub in df.groupby('file'):\n",
        "    # Check controlling questions\n",
        "    a62x_noncompliant = sub[sub[\"question_id\"].isin([\"A6.2.1\", \"A6.2.2\", \"A6.2.3\", \"A6.2.4\"])][\"compliance\"].eq(\"Non-Compliant\").any()\n",
        "    # Check dependent questions\n",
        "    for qid in [\"A6.2\", \"A6.6\"]:\n",
        "        label = sub.loc[sub[\"question_id\"] == qid, \"compliance\"]\n",
        "        if a62x_noncompliant and not label.empty and label.iloc[0] == \"Compliant\":\n",
        "            violations.append((file, qid, \"Should be Non-Compliant due to A6.2.x\"))\n",
        "\n",
        "print(f\"Violations (dependency broken) in manual annotation: {len(violations)}\")\n",
        "for v in violations[:10]:\n",
        "    print(v)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8MfHUU1KDxp"
      },
      "source": [
        "### - A4.5.1 or A4.6 → A4.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV00RQmaKGp9"
      },
      "source": [
        "If A4.5.1 OR A4.6 is Non-Compliant in a form, then A4.5 should also be Non-Compliant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFGnTNkaomso"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/project/CB/manual_annotated_questions.csv\", dtype=str).fillna(\"\")\n",
        "\n",
        "violations_a45 = []\n",
        "\n",
        "for file, sub in df.groupby('file'):\n",
        "    # Is either controller non-compliant?\n",
        "    nc_ctrl = sub[sub[\"question_id\"].isin([\"A4.5.1\", \"A4.6\"])][\"compliance\"].eq(\"Non-Compliant\").any()\n",
        "    # What is the dependent label?\n",
        "    dep_lab = sub.loc[sub[\"question_id\"] == \"A4.5\", \"compliance\"]\n",
        "    if nc_ctrl and not dep_lab.empty and dep_lab.iloc[0] == \"Compliant\":\n",
        "        violations_a45.append((file, \"A4.5\", \"Should be Non-Compliant due to A4.5.1 or A4.6\"))\n",
        "\n",
        "print(f\"\\nA4.5 dependency violations in manual annotation: {len(violations_a45)}\")\n",
        "for v in violations_a45[:10]:\n",
        "    print(v)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZk0cWxpJ63C"
      },
      "source": [
        "### - A7.14 == \"No\" → A7.16 / A7.17"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nj4-ngG6J2Wp"
      },
      "source": [
        "If A7.14 answer is \"no\" or \"n\", then A7.16 and A7.17 should both be Non-Compliant.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDwJXo_Uo1Tf"
      },
      "outputs": [],
      "source": [
        "violations_a714 = []\n",
        "\n",
        "for file, sub in df.groupby('file'):\n",
        "    a714_ans = sub.loc[sub[\"question_id\"] == \"A7.14\", \"answer_text\"]\n",
        "    if not a714_ans.empty and a714_ans.iloc[0].strip().lower() in {\"no\", \"n\"}:\n",
        "        for qid in [\"A7.16\", \"A7.17\"]:\n",
        "            dep_lab = sub.loc[sub[\"question_id\"] == qid, \"compliance\"]\n",
        "            if not dep_lab.empty and dep_lab.iloc[0] == \"Compliant\":\n",
        "                violations_a714.append((file, qid, \"Should be Non-Compliant due to A7.14=='No'\"))\n",
        "\n",
        "print(f\"\\nA7.14 dependency violations in manual annotation: {len(violations_a714)}\")\n",
        "for v in violations_a714[:10]:\n",
        "    print(v)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6QL10gLkg5T"
      },
      "source": [
        "### - Cross-Question Dependency Rules (Firewall, External Services, Patch Process, Isolation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5GHL--shRw0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "CSV_PATH = \"/content/drive/MyDrive/project/CB/manual_annotated_questions.csv\"\n",
        "df = pd.read_csv(CSV_PATH, dtype=str).fillna(\"\")\n",
        "\n",
        "# Normalise helper ----------------------------------------------------------\n",
        "def norm(s):\n",
        "    return str(s).strip().lower()\n",
        "\n",
        "df[\"answer_text_norm\"] = df[\"answer_text\"].apply(norm)\n",
        "df[\"compliance_norm\"] = df[\"compliance\"].apply(norm)\n",
        "\n",
        "# Build a form-level pivot: file  ×  question_id  →  row ---------------------\n",
        "forms = (\n",
        "    df.groupby(\"file\")\n",
        "      .apply(lambda g: g.set_index(\"question_id\").to_dict(orient=\"index\"))\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Rule helpers\n",
        "# ---------------------------------------------------------------------------\n",
        "def is_yes(row):      # yes/no questions\n",
        "    return norm(row.get(\"answer_text\", \"\")) in {\"yes\", \"y\", \"true\", \"1\"}\n",
        "def is_no(row):\n",
        "    return norm(row.get(\"answer_text\", \"\")) in {\"no\", \"n\", \"false\", \"0\"}\n",
        "def is_noncompliant(row):\n",
        "    return norm(row.get(\"compliance\", \"\")) == \"non-compliant\"\n",
        "\n",
        "# Store violations here\n",
        "violations = {f\"Rule{i}\": [] for i in range(1, 6)}\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "for file_name, qdict in forms.items():\n",
        "    # -----------------------------------------------------------------------\n",
        "\n",
        "    # -----------------------------------------------------------------------\n",
        "    # Rule-1  A4.8 → (A4.9 + A4.10)\n",
        "    row_a48 = qdict.get(\"A4.8\")\n",
        "    row_a49 = qdict.get(\"A4.9\")\n",
        "    row_a410 = qdict.get(\"A4.10\")\n",
        "    if row_a48 and is_yes(row_a48):\n",
        "        if not row_a49 or is_noncompliant(row_a49) or not row_a410 or is_noncompliant(row_a410):\n",
        "            violations[\"Rule2\"].append(file_name)\n",
        "\n",
        "    # -----------------------------------------------------------------------\n",
        "    # Rule-2  A5.4 → A5.5 & A5.6 & A5.7\n",
        "    row_a54 = qdict.get(\"A5.4\")\n",
        "    row_a55 = qdict.get(\"A5.5\")\n",
        "    row_a56 = qdict.get(\"A5.6\")\n",
        "    row_a57 = qdict.get(\"A5.7\")\n",
        "    if row_a54 and is_yes(row_a54):\n",
        "        if any(r is None or is_noncompliant(r) for r in (row_a55, row_a56, row_a57)):\n",
        "            violations[\"Rule3\"].append(file_name)\n",
        "\n",
        "    # -----------------------------------------------------------------------\n",
        "    # Rule-3  (A6.4.1 → A6.4.2)  and  (A6.5.1 → A6.5.2)\n",
        "    for base in [\"A6.4\", \"A6.5\"]:\n",
        "        r1 = qdict.get(f\"{base}.1\")   # auto-update enabled?\n",
        "        r2 = qdict.get(f\"{base}.2\")   # manual process described?\n",
        "        if r1 and is_no(r1):          # auto-updates disabled\n",
        "            if r2 is None or is_noncompliant(r2):\n",
        "                violations[\"Rule4\"].append(file_name)\n",
        "\n",
        "    # -----------------------------------------------------------------------\n",
        "    # Rule-4  A6.7 isolation claim ↔ A2.2 scope description\n",
        "    row_a67 = qdict.get(\"A6.7\")\n",
        "    row_a22 = qdict.get(\"A2.2\")\n",
        "    if row_a67 and is_yes(row_a67):   # claims unsupported SW isolated\n",
        "        # Heuristic: A2.2 scope must mention \"sub-net\", \"vlan\", or \"segment\"\n",
        "        scope_ok = row_a22 and any(k in row_a22[\"answer_text_norm\"]\n",
        "                                   for k in (\"subnet\", \"vlan\", \"segment\", \"separate\"))\n",
        "        if not scope_ok:\n",
        "            violations[\"Rule5\"].append(file_name)\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Print results -------------------------------------------------------------\n",
        "print(\"\\n=== Dependency-rule violations ===\")\n",
        "for rule, files in violations.items():\n",
        "    print(f\"{rule}: {len(files)} forms violated\")\n",
        "    if files:\n",
        "        print(\"   \", files[:10], \"...\" if len(files) > 10 else \"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDCmhwwGkPsD"
      },
      "source": [
        "All dependency rules were followed correctly — no forms violated any of them. This means that whenever a form gave a risky answer (like exposing a firewall admin page or turning off auto-updates), it also included the necessary follow-up details and protections, such as MFA, patching routines, or subnet isolation. In simple terms, the applicants who reported higher-risk setups also showed they had the right controls in place to manage those risks. This shows good understanding and consistent answers across forms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og6pCK7lM7MV"
      },
      "source": [
        "### **HYPOTHESES TESTING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAY_4ddX4cbM"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbsQL62t5eDL"
      },
      "source": [
        "***Hypothesis 1:***\n",
        "***Descriptive answers ≥30 words are ≥3× more likely to be Compliant than ≤15 words.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVNnYLRzkyL3"
      },
      "source": [
        "Since I am only comparing two proportions (compliance rates between short and long answers) then its appropriate to use\n",
        "\n",
        "\n",
        ":Two-proportion z-test when expected cell counts ≥ 5.\n",
        "\n",
        "Fisher’s Exact Test as a fallback for small counts.(as Fallback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJY_Ii84DWqu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.stats.proportion import proportions_ztest, proportion_confint\n",
        "from scipy.stats import fisher_exact\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 1.  Prepare Descriptive subset\n",
        "# -------------------------------------------------------------\n",
        "DESC_IDS = {\n",
        "    \"A4.1.1\",\"A4.10\",\"A4.2.1\",\"A4.6\",\"A5.1\",\"A5.10\",\"A5.6\",\n",
        "    \"A6.4.2\",\"A6.5.2\",\"A6.7\",\"A7.1\",\"A7.10\",\"A7.11\",\"A7.12\",\n",
        "    \"A7.3\",\"A7.4\",\"A7.5\",\"A7.6\",\"A7.7\"\n",
        "}\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/project/CB/manual_annotated_questions.csv\",\n",
        "                 dtype=str).fillna(\"\")\n",
        "\n",
        "desc = df[df[\"question_id\"].isin(DESC_IDS)].copy()\n",
        "desc[\"word_count\"] = desc[\"answer_text\"].str.split().str.len()\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2.  Label short (≤15) vs long (≥30)\n",
        "# -------------------------------------------------------------\n",
        "desc[\"wc_bin\"] = pd.cut(\n",
        "    desc[\"word_count\"],\n",
        "    bins=[-1, 15, 29, np.inf],\n",
        "    labels=[\"short\", \"medium\", \"long\"],\n",
        "    right=True\n",
        ")\n",
        "\n",
        "# Descriptive table (optional)\n",
        "print(\"\\nWord-count distribution by label:\\n\",\n",
        "      pd.crosstab(desc[\"wc_bin\"], desc[\"compliance\"]), \"\\n\")\n",
        "\n",
        "# Focus on short vs long only\n",
        "mask = desc[\"wc_bin\"].isin([\"short\", \"long\"])\n",
        "short_long = desc[mask].copy()\n",
        "short_long[\"is_compliant\"] = short_long[\"compliance\"].eq(\"Compliant\")\n",
        "\n",
        "# Counts\n",
        "long_comp  = short_long[(short_long[\"wc_bin\"] == \"long\")  & short_long[\"is_compliant\"]].shape[0]\n",
        "long_total = short_long[short_long[\"wc_bin\"] == \"long\"].shape[0]\n",
        "short_comp = short_long[(short_long[\"wc_bin\"] == \"short\") & short_long[\"is_compliant\"]].shape[0]\n",
        "short_total= short_long[short_long[\"wc_bin\"] == \"short\"].shape[0]\n",
        "\n",
        "table = np.array([[long_comp,  long_total - long_comp],   # row 0 = long\n",
        "                  [short_comp, short_total - short_comp]],dtype=float)\n",
        "\n",
        "# Haldane–Anscombe if zero\n",
        "if (table == 0).any():\n",
        "    table += 0.5\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 3.  Odds-ratio + 95 % CI\n",
        "# -------------------------------------------------------------\n",
        "OR  = (table[0,0] * table[1,1]) / (table[0,1] * table[1,0])\n",
        "SE  = np.sqrt(np.sum(1/table))\n",
        "ci_low, ci_high = np.exp(np.log(OR) + np.array([-1.96, 1.96])*SE)\n",
        "\n",
        "print(f\"Compliant rate  (long ≥30) : {long_comp/long_total:.2%}  ({long_comp}/{long_total})\")\n",
        "print(f\"Compliant rate  (short ≤15): {short_comp/short_total:.2%}  ({short_comp}/{short_total})\")\n",
        "print(f\"Odds-ratio (long / short)  : {OR:.2f}  (95 % CI {ci_low:.2f}–{ci_high:.2f})\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 4.  Significance test  (choose z-test or Fisher)\n",
        "# -------------------------------------------------------------\n",
        "# Use z-test if all expected ≥5\n",
        "exp = (table.sum() *\n",
        "       np.outer(table.sum(axis=1), table.sum(axis=0)) /\n",
        "       table.sum()**2)\n",
        "if (exp < 5).any():\n",
        "    p_val = fisher_exact(table)[1]\n",
        "    test_used = \"Fisher’s Exact\"\n",
        "else:\n",
        "    count = [long_comp, short_comp]\n",
        "    nobs  = [long_total, short_total]\n",
        "    _, p_val = proportions_ztest(count, nobs)\n",
        "    test_used = \"Two-sample z-test\"\n",
        "\n",
        "print(f\"{test_used} p-value         : {p_val:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvBObj0dzUBv"
      },
      "source": [
        "Narrative responses containing at least 30 tokens show a dramatically higher probability of compliance (96 %) compared with answers of 15 tokens or fewer (60 %). The odds-ratio of 17.2 (95 % CI 10.3–28.6) and a z-test p-value below 0.001 provide decisive evidence that longer descriptive answers are strongly associated with favourable assessments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUfWl3zM5VH-"
      },
      "source": [
        "***Hypothesis 2: For List/Semantic questions, word-count does not predict compliance.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1GLzEv-jHlo"
      },
      "source": [
        "For comparing proportions (compliance rates) between two groups, I am expecting to get binary outcome (Compliant vs Non-Compliant) so only z-test or Fisher’s Exact Test is appropriate which assume binomial distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nydOU6RZgjci"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzhXjc3YJf7O"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import chi2_contingency, fisher_exact\n",
        "from statsmodels.stats.proportion import proportions_ztest, proportion_confint\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1 .  Load data and isolate List / Semantic questions\n",
        "# ---------------------------------------------------------\n",
        "CSV_PATH = \"/content/drive/MyDrive/project/CB/manual_annotated_questions.csv\"\n",
        "df = pd.read_csv(CSV_PATH, dtype=str).fillna(\"\")\n",
        "\n",
        "LIST_IDS = {\n",
        "    \"A2.3\",\"A2.4\",\"A2.4.1\",\"A2.5\",\"A2.6\",\"A2.7\",\"A2.7.1\",\n",
        "    \"A2.8\",\"A2.9\",\"A4.12\",\"A6.2.1\",\"A6.2.2\",\"A6.2.3\",\"A6.2.4\"\n",
        "}\n",
        "ls = df[df[\"question_id\"].isin(LIST_IDS)].copy()\n",
        "ls[\"word_count\"]   = ls[\"answer_text\"].str.split().str.len()\n",
        "ls[\"is_compliant\"] = ls[\"compliance\"].eq(\"Compliant\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2 .  Flag “short (< 10)”  and  “long (> 25)”\n",
        "# ---------------------------------------------------------\n",
        "ls[\"len_flag\"] = np.select(\n",
        "    [ls[\"word_count\"] < 10, ls[\"word_count\"] > 25],\n",
        "    [\"short\", \"long\"],\n",
        "    default=\"mid\"\n",
        ")\n",
        "\n",
        "# Keep only short vs long rows\n",
        "sl = ls[ls[\"len_flag\"].isin([\"short\", \"long\"])]\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3 .  Build 2 × 2 contingency table\n",
        "# ---------------------------------------------------------\n",
        "short_bad = sl[(sl[\"len_flag\"]==\"short\") & (~sl[\"is_compliant\"])].shape[0]\n",
        "short_tot = sl[ sl[\"len_flag\"]==\"short\"].shape[0]\n",
        "long_bad  = sl[(sl[\"len_flag\"]==\"long\")  & (~sl[\"is_compliant\"])].shape[0]\n",
        "long_tot  = sl[ sl[\"len_flag\"]==\"long\"].shape[0]\n",
        "\n",
        "# (rows = long / short, cols = compliant / non-compliant)\n",
        "table = np.array([[long_tot-long_bad, long_bad],\n",
        "                  [short_tot-short_bad, short_bad]], dtype=float)\n",
        "\n",
        "# Haldane–Anscombe if any zero\n",
        "if (table == 0).any():\n",
        "    table += 0.5\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4 .  Compliance rates, odds-ratio, 95 % CI\n",
        "# ---------------------------------------------------------\n",
        "rate_long  = (long_tot-long_bad) / long_tot\n",
        "rate_short = (short_tot-short_bad) / short_tot\n",
        "OR  = (table[0,0]*table[1,1]) / (table[0,1]*table[1,0])\n",
        "SE  = np.sqrt(np.sum(1/table))\n",
        "ci_low, ci_high = np.exp(np.log(OR) + np.array([-1.96, 1.96])*SE)\n",
        "\n",
        "print(f\"Compliance rate  (<10 words) : {rate_short:.2%}  ({short_tot-short_bad}/{short_tot})\")\n",
        "print(f\"Compliance rate  (>25 words) : {rate_long :.2%}  ({long_tot-long_bad}/{long_tot})\")\n",
        "print(f\"Odds-ratio (long / short)    : {OR:.2f}  (95 % CI {ci_low:.2f}–{ci_high:.2f})\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 5 .  Significance test  –  Fisher if expected < 5\n",
        "# ---------------------------------------------------------\n",
        "exp = (table.sum()*np.outer(table.sum(1), table.sum(0))/table.sum()**2)\n",
        "if (exp < 5).any():\n",
        "    p_val = fisher_exact(table)[1]\n",
        "    test = \"Fisher’s Exact\"\n",
        "else:\n",
        "    count = [long_tot-long_bad, short_tot-short_bad]\n",
        "    nobs  = [long_tot, short_tot]\n",
        "    p_val = proportions_ztest(count, nobs)[1]\n",
        "    test = \"Two-proportion z-test\"\n",
        "\n",
        "print(f\"{test} p-value               : {p_val:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckwIwoNf0nts"
      },
      "source": [
        "Long list-style responses (> 25 words) have a substantially higher chance of receiving a Compliant label compared with ultra-short responses (< 10 words).\n",
        "The observed odds-ratio (4.42) far exceeds the “≥ 3 ×” benchmark, and the effect is statistically decisive (p < 0.001)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LokRbj9yLRE-"
      },
      "source": [
        "***H3: The proportion of stop-words is a significant predictor of the Non-Compliant label in List/Semantic questions, but shows little to no predictive power in Descriptive questions.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lDkLX4RgoUs"
      },
      "source": [
        "**Assumptions:**\n",
        "\n",
        "\n",
        "Logistic: binary outcome, log-odds can be modelled as a linear function of stop_pct.\n",
        "\n",
        "Mann-Whitney: stop_pct is ordinal/continuous but non-normal, so a rank test is suitable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Pg5zVJkBRm_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd, numpy as np\n",
        "import statsmodels.formula.api as smf\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 1 .  Load data\n",
        "# ----------------------------------------------------------\n",
        "CSV = \"/content/drive/MyDrive/project/CB/manual_annotated_questions.csv\"\n",
        "df  = pd.read_csv(CSV, dtype=str).fillna(\"\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 2 .  Define lists of IDs\n",
        "# ----------------------------------------------------------\n",
        "DESC_IDS = {\n",
        "    \"A4.1.1\",\"A4.10\",\"A4.2.1\",\"A4.6\",\"A5.1\",\"A5.10\",\"A5.6\",\n",
        "    \"A6.4.2\",\"A6.5.2\",\"A6.7\",\"A7.1\",\"A7.10\",\"A7.11\",\"A7.12\",\n",
        "    \"A7.3\",\"A7.4\",\"A7.5\",\"A7.6\",\"A7.7\"\n",
        "}\n",
        "LIST_IDS = {\n",
        "    \"A2.3\",\"A2.4\",\"A2.4.1\",\"A2.5\",\"A2.6\",\"A2.7\",\"A2.7.1\",\n",
        "    \"A2.8\",\"A2.9\",\"A4.12\",\"A6.2.1\",\"A6.2.2\",\"A6.2.3\",\"A6.2.4\"\n",
        "}\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 3 .  Compute stop-word percentage\n",
        "# ----------------------------------------------------------\n",
        "STOP = set(ENGLISH_STOP_WORDS)\n",
        "def stop_pct(text):\n",
        "    toks = [w for w in str(text).lower().split() if w.isalpha()]\n",
        "    return 0.0 if not toks else sum(t in STOP for t in toks) / len(toks)\n",
        "\n",
        "df[\"stop_pct\"] = df[\"answer_text\"].apply(stop_pct)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 4 .  Assign qtype without mixing float NaN and strings\n",
        "# ----------------------------------------------------------\n",
        "df[\"qtype\"] = pd.NA                    # start with pandas NA (object dtype)\n",
        "df.loc[df[\"question_id\"].isin(DESC_IDS), \"qtype\"] = \"Desc\"\n",
        "df.loc[df[\"question_id\"].isin(LIST_IDS),  \"qtype\"] = \"List\"\n",
        "df = df[df[\"qtype\"].notna()].copy()    # keep only Desc & List rows\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 5 .  Binary outcome: Non-Compliant vs other\n",
        "# ----------------------------------------------------------\n",
        "df[\"is_nc\"] = df[\"compliance\"].eq(\"Non-Compliant\").astype(int)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 6 .  Logistic regression with interaction\n",
        "# ----------------------------------------------------------\n",
        "model = smf.logit(\"is_nc ~ stop_pct * C(qtype)\", data=df).fit(disp=False)\n",
        "print(model.summary())\n",
        "\n",
        "#  β_interaction (stop_pct:C(qtype)[T.List]) tells if slopes differ.\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 7 .  Simple-effects Mann-Whitney tests\n",
        "# ----------------------------------------------------------\n",
        "d_list = df[df[\"qtype\"] == \"List\"]\n",
        "d_desc = df[df[\"qtype\"] == \"Desc\"]\n",
        "\n",
        "u_list, p_list = mannwhitneyu(\n",
        "    d_list.loc[d_list[\"is_nc\"]==1, \"stop_pct\"],\n",
        "    d_list.loc[d_list[\"is_nc\"]==0, \"stop_pct\"],\n",
        "    alternative=\"two-sided\"\n",
        ")\n",
        "u_desc, p_desc = mannwhitneyu(\n",
        "    d_desc.loc[d_desc[\"is_nc\"]==1, \"stop_pct\"],\n",
        "    d_desc.loc[d_desc[\"is_nc\"]==0, \"stop_pct\"],\n",
        "    alternative=\"two-sided\"\n",
        ")\n",
        "\n",
        "print(f\"\\nSimple-effect tests\")\n",
        "print(f\"  List  : Mann-Whitney p = {p_list:.4f}\")\n",
        "print(f\"  Desc  : Mann-Whitney p = {p_desc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFqHUs_bLHhf"
      },
      "source": [
        "A logistic regression with an interaction term (stop_pct × qtype) showed neither a main effect of stop-word proportion (β = 0.50, p = 0.43) nor an interaction with question type (β = –0.67, p = 0.76). Mann-Whitney tests within each question type corroborated these null findings (p > 0.85). We therefore conclude that stop-word density does not influence compliance assessment in either Descriptive or List/Semantic answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH7WF7sB7naW"
      },
      "source": [
        "***H4: The More-Information-Required (MIR) rate across the software-inventory questions (A6.2.1 to A6.2.4) is at least twice as high as the average MIR rate observed across all other questions in the form.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2aLD62AdoPM"
      },
      "source": [
        "**Assumptions satisfied**\n",
        "\n",
        "\n",
        "Independence — Each form appears once per question; answers are independent across forms.\n",
        "\n",
        "Fixed categories — MIR vs not-MIR, cluster vs other.\n",
        "\n",
        "Large-sample or exact — Your code checks expected counts and switches to Fisher when needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aUtX1XuJz4t"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import chi2_contingency, fisher_exact\n",
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# 1. Load data\n",
        "# -----------------------------------------------------------------\n",
        "CSV = \"/content/drive/MyDrive/project/CB/manual_annotated_questions.csv\"\n",
        "df  = pd.read_csv(CSV, dtype=str).fillna(\"\")\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# 2. Define the inventory-cluster and flag MIR outcomes\n",
        "# -----------------------------------------------------------------\n",
        "INV_IDS = {\"A6.2.1\",\"A6.2.2\", \"A6.2.3\", \"A6.2.4\"}   # software-inventory cluster\n",
        "\n",
        "df[\"is_inv\"]   = df[\"question_id\"].isin(INV_IDS)\n",
        "df[\"is_mir\"]   = df[\"compliance\"].eq(\"More-information\")\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# 3. Build 2 × 2 contingency table\n",
        "#    rows: cluster / other, cols: MIR / not-MIR\n",
        "# -----------------------------------------------------------------\n",
        "ct = pd.crosstab(df[\"is_inv\"], df[\"is_mir\"]).reindex(\n",
        "        index=[False, True], columns=[False, True], fill_value=0)\n",
        "\n",
        "cluster_mir = ct.loc[True,  True]\n",
        "cluster_tot = ct.loc[True].sum()\n",
        "other_mir   = ct.loc[False, True]\n",
        "other_tot   = ct.loc[False].sum()\n",
        "\n",
        "rate_cluster = cluster_mir / cluster_tot\n",
        "rate_other   = other_mir   / other_tot\n",
        "ratio        = rate_cluster / rate_other if rate_other else float(\"inf\")\n",
        "\n",
        "print(\"2 × 2 table (rows: cluster / other, cols: MIR / not-MIR):\\n\", ct, \"\\n\")\n",
        "print(f\"MIR-rate cluster  = {rate_cluster:.3%}  ({cluster_mir}/{cluster_tot})\")\n",
        "print(f\"MIR-rate other    = {rate_other:.3%}  ({other_mir}/{other_tot})\")\n",
        "print(f\"Rate ratio        = {ratio:.2f}×\\n\")\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# 4. Choose appropriate test\n",
        "# -----------------------------------------------------------------\n",
        "chi2, p_chi, _, exp = chi2_contingency(ct, correction=False)\n",
        "if (exp < 5).any():\n",
        "    p_val = fisher_exact(ct)[1]\n",
        "    test  = \"Fisher’s Exact\"\n",
        "else:\n",
        "    count = [cluster_mir, other_mir]\n",
        "    nobs  = [cluster_tot,  other_tot]\n",
        "    stat, p_val = proportions_ztest(count, nobs, alternative=\"larger\")\n",
        "    test  = \"One-sided two-proportion z-test\"\n",
        "\n",
        "print(f\"{test}  p-value = {p_val:.4f}\")\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# 5. Decision logic\n",
        "# -----------------------------------------------------------------\n",
        "if ratio >= 2 and p_val < 0.05:\n",
        "    print(\"\\n Hypothesis supported: cluster MIR rate is ≥ 2× baseline and significant.\")\n",
        "else:\n",
        "    print(\"\\n Hypothesis not supported: either < 2× or not significant.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG7UaDga5NqV"
      },
      "source": [
        "- The software-inventory quartet (e.g., questions requiring lists of installed or licensed software) is approximately forty times more likely to trigger a More-Information-Required (MIR) label compared to other parts of the form—a difference that is statistically overwhelming. This suggests a structural challenge in these questions: they often implicitly require detailed responses (e.g., software names and version numbers), but applicants frequently omit such specifics.\n",
        "\n",
        "- To ensure annotation consistency, MIR labelling for these semantic-list questions must be tightly aligned with manual annotation criteria."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}